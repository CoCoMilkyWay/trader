{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c21c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first use Tsfresh/Tsfel to generate some features to get a feel of what we are dealing with:\n",
    "#   1. Tsfresh takes n*(id, time, features) and len(id) labels as input:\n",
    "#   2. tsfresh doesn't test/preprocess anything about input timeseries, so you better make them stationary by yourselves\n",
    "#   3. id = independent timeseries, long or short\n",
    "#   4. each id has 1 label, which is against the notion that timeseries need to have timeseries label as well\n",
    "#   5. however, at each time stamp, we have short memory features and long memory features, for short memory features, it is actually equivalent to having a sliding window of short timeseries and output a series of scalar labels, which is exactly how tsfresh/tsfel works\n",
    "#   6. indeed, it might be the best practice to use these tools to generate short memory features and handcraft long memory/more hidden features\n",
    "#   7. these tools can not catch any cross-sectional features between ids. you can generate features for 1 id at a time, works exactly the same, it is just more computationally efficient for parallelism\n",
    "#   8. do features on long timeseries works equally well on its splitted many rolling short timeseries on average? not necessary\n",
    "#   9. thus how do you evaluate whether features generated like this work consistently over time? either averaging statistical performance or try to train a model(then compare model weights)\n",
    "#   10.try short->long window for feature importance(e.g. FFT doesn't work well on short window length)\n",
    "#   11.to evaluate the effect of a feature, dont need to have too many, have enough samples that can make sure feature is stationary and preferably normal distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca03dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | **Method**                                       | **Approach Type**           | **Pattern Type Learned**                 | **Interpretable?** | **Best Use Case**                                  | **Tools / Libraries**                |\n",
    "# | ------------------------------------------------ | --------------------------- | ---------------------------------------- | ------------------ | -------------------------------------------------- | ------------------------------------ |\n",
    "# | **Shapelet Transform**                           | Distance-based (Supervised) | Local subsequence \"shapes\"               | ✅ High            | Finding interpretable, discriminative patterns     | `tslearn`, `sktime`, `pyts`          |\n",
    "# | **Dynamic Time Warping + Supervised Clustering** | Similarity + Aggregation    | Whole series similarity (flexible time)  | ✅ Medium          | Pattern grouping + average label scoring           | `tslearn`, `dtaidistance`, `HDBSCAN` |\n",
    "# | **Bag-of-SFA Symbols (BOSS)**                    | Symbolic / Frequency        | Frequency of symbolic subsequences       | ✅ High            | Symbolic pattern detection (e.g., zigzags)         | `sktime`, `pyts`                     |\n",
    "# | **TDE (Temporal Dictionary Ensemble)**           | Dictionary Learning         | Frequency/strength of learned shapes     | ✅ High            | Detecting repeated motifs with outcome correlation | `sktime`                             |\n",
    "# | **Time Series Forest (TSF)**                     | Tree Ensemble               | Random intervals + summaries             | ⚠️ Partial         | Strong classification baseline                     | `sktime`, `tslearn`                  |\n",
    "# | **HIVE-COTE 2.0**                                | Ensemble (Hybrid)           | Multiple feature types                   | ✅ Partial         | State-of-the-art accuracy on many tasks            | `sktime`                             |\n",
    "# | **ROCKET / MiniROCKET / MultiROCKET**            | Random Kernels              | Statistical response to many filters     | ❌ No              | Fast, accurate classification/regression           | `sktime`, `rocket-boost`             |\n",
    "# | **1D CNNs**                                      | Deep Learning (CNN)         | Localized filters (motifs)               | ⚠️ Limited         | Predicting from raw time series                    | `Keras`, `PyTorch`                   |\n",
    "# | **RNNs / LSTMs / Transformers**                  | Deep Learning (Sequential)  | Temporal dynamics, memory, long patterns | ❌ No              | Capturing long-term dependencies                   | `PyTorch`, `Keras`, `Hugging Face`   |\n",
    "# | **Siamese / Triplet Networks**                   | Metric Learning             | Latent similarity between time windows   | ⚠️ Medium          | Learning similarity among high-performing windows  | `PyTorch`, `TensorFlow`              |\n",
    "# | **Autoencoder + Regressor**                      | Latent Representation       | Abstract embeddings                      | ⚠️ Medium          | Unsupervised pretraining + label prediction        | `PyTorch`, `scikit-learn`            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe72910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple features\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label.parquet'))\n",
    "raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label_trend_1to8.parquet'))\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label_trend_3to24.parquet'))\n",
    "raw_data = raw_data[-(60*24*14):].copy()\n",
    "\n",
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close'] / raw_data['close'].shift(1))\n",
    "\n",
    "# Momentum\n",
    "for i in range(1, 6):\n",
    "    raw_data[f'mom{i}'] = raw_data['close'].pct_change(periods=i)\n",
    "\n",
    "# Volatility\n",
    "window_stdev = 50\n",
    "raw_data['volatility'] = raw_data['log_ret'].rolling(window=window_stdev, min_periods=window_stdev).std()\n",
    "\n",
    "# Serial Correlation\n",
    "window_autocorr = 50\n",
    "for lag in range(1, 6):\n",
    "    raw_data[f'autocorr_{lag}'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr).apply(\n",
    "        lambda x: x.autocorr(lag=lag) if x.notna().sum() > lag else np.nan,\n",
    "        raw=False\n",
    "    )\n",
    "\n",
    "# Lagged log returns\n",
    "for i in range(1, 6):\n",
    "    raw_data[f'log_t{i}'] = raw_data['log_ret'].shift(i)\n",
    "\n",
    "# Moving averages\n",
    "fast_window = 7\n",
    "slow_window = 15\n",
    "raw_data['fast_mavg'] = raw_data['close'].rolling(window=fast_window, min_periods=fast_window).mean()\n",
    "raw_data['slow_mavg'] = raw_data['close'].rolling(window=slow_window, min_periods=slow_window).mean()\n",
    "\n",
    "# ATR\n",
    "high_low = raw_data['high'] - raw_data['low']\n",
    "high_close = (raw_data['high'] - raw_data['close'].shift()).abs()\n",
    "low_close = (raw_data['low'] - raw_data['close'].shift()).abs()\n",
    "tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "raw_data['atr'] = tr.ewm(span=12, adjust=False).mean()\n",
    "\n",
    "# raw_data.info(verbose=True, memory_usage='deep')\n",
    "raw_data.describe().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf622116",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3800220679.py, line 223)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmaxium likelihood\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Tsresh Features\n",
    "# https://tsfresh.readthedocs.io/en/latest/text/list_of_features.html\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# print in full\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# period < 20 = stationary (ADF test)\n",
    "# period > 10 = significant trend info (correlation)\n",
    "\n",
    "M = 200 # number of top_features kept\n",
    "weeks = 1\n",
    "period = 15\n",
    "step = 15 # should be 1\n",
    "\n",
    "dir = os.path.join(os.getcwd())\n",
    "# dir = os.path.dirname(os.path.abspath(__file__))\n",
    "filepath = os.path.join(dir, f\"bar_and_label.parquet\")\n",
    "\n",
    "# 'bar_and_label.parquet'\n",
    "# 'bar_and_label_trend_1to8.parquet'\n",
    "# 'bar_and_label_trend_3to24.parquet'\n",
    "\n",
    "def analyze_feature(df: pd.DataFrame):\n",
    "    series = df[\"close\"]\n",
    "    df['ref'] = np.log((series / series.iloc[0]).fillna(1))\n",
    "    df['value'] = np.log((series / series.rolling(period).mean().shift(period)).fillna(1))\n",
    "    \n",
    "    # print(df.tail())\n",
    "    df = df[-int(60/5*24*7*weeks):].copy()\n",
    "    df = df.sort_values('time').reset_index()\n",
    "    \n",
    "    # Rolling the time series\n",
    "    window_size = period\n",
    "    step_size = step\n",
    "    segments = []\n",
    "    labels = []\n",
    "    times = []\n",
    "    for idx, start in enumerate(range(0, len(df) - window_size, step_size)):\n",
    "        end = start + window_size\n",
    "        segment = df.iloc[start:end].copy()\n",
    "        segment[\"id\"] = idx  # unique id for each window\n",
    "        segments.append(segment[[\"id\", \"time\", \"value\"]])\n",
    "        labels.append(df[\"label\"].iloc[end])\n",
    "        times.append(df[\"time\"].iloc[end])\n",
    "\n",
    "    X_features = pd.concat(segments, axis=0)\n",
    "    y_labels = pd.Series(labels)\n",
    "    t_times = pd.Series(times)\n",
    "\n",
    "    print(X_features.tail())\n",
    "    # print(y_labels.tail())\n",
    "\n",
    "    # --- Feature extraction\n",
    "    X_features = extract_features(\n",
    "        X_features,\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"time\",\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        show_warnings=False,\n",
    "        impute_function=impute, # NOTE: tsfresh's impute implementation may leak future info, but mostly okay\n",
    "        # disable_progressbar=True\n",
    "    )\n",
    "    X_features = pd.DataFrame(X_features)  # Ensure features is a DataFrame\n",
    "    X_features = X_features.loc[:, X_features.notna().sum() == len(X_features)] # remove NaN features after impute\n",
    "    X_features = X_features.loc[:, X_features.nunique() > 1] # remove constant features\n",
    "    print(f\"Valid features number:{X_features.shape[1]}\")\n",
    "    # X_features = select_features(X_features, y_labels)\n",
    "    # print(X_features.describe().transpose())\n",
    "\n",
    "    # | Feature Type | Target Type | Statistical Test Used             |\n",
    "    # | ------------ | ----------- | --------------------------------- |\n",
    "    # | Continuous   | Binary      | Kolmogorov-Smirnov test (KS test) |\n",
    "    # | Continuous   | Categorical | ANOVA F-test                      |\n",
    "    # | Continuous   | Continuous  | Kendall's tau correlation test    |\n",
    "    # | Binary       | Binary      | Fisher’s exact test               |\n",
    "    # | Binary       | Categorical | Chi-square test                   |\n",
    "    # | Binary       | Continuous  | Point biserial correlation        |\n",
    "\n",
    "    # --- Compute correlation with label\n",
    "    results = {}\n",
    "    for col in X_features.columns:\n",
    "        # Use Spearman for robustness (nonlinear monotonic relationships)\n",
    "        corr = pd.Series(X_features[col]).corr(y_labels, method='spearman')\n",
    "        results[col] = abs(corr)  # use absolute value to reflect strength\n",
    "    \n",
    "    # --- Sort by importance\n",
    "    sorted_features = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop statistically important features (Spearman correlation):\")\n",
    "    for name, score in sorted_features[:10]:\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "    \n",
    "    top_features = [name for name, score in sorted_features[:M]]\n",
    "    X_selected = X_features[top_features].copy()\n",
    "    X_selected[\"time\"] = t_times.values\n",
    "    X_selected[\"label\"] = y_labels.values\n",
    "    X_selected.set_index('time', inplace=True)\n",
    "    X_selected.to_parquet(os.path.join(dir, f\"tsfresh_features_and_label_{weeks}weeks.parquet\"))\n",
    "    # print(X_selected.describe().transpose())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_parquet(filepath)\n",
    "    analyze_feature(df)\n",
    "\n",
    "\"\"\"\n",
    "Top statistically important features (Spearman correlation):\n",
    "close__has_duplicate_max: 0.0721\n",
    "close__spkt_welch_density__coeff_5: 0.0693\n",
    "close__symmetry_looking__r_0.30000000000000004: 0.0532\n",
    "close__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.4: 0.0521\n",
    "close__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.6: 0.0505 \n",
    "close__large_standard_deviation__r_0.4: 0.0477\n",
    "close__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"var\": 0.0474  \n",
    "close__partial_autocorrelation__lag_4: 0.0434\n",
    "close__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.2__ql_0.0: 0.0420\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
