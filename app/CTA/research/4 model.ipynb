{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# We consider family of random forest as baseline \"Feature-Based Time Series Models\" here:\n",
    "# =============================================================================\n",
    "# | Model                    | Year  | Core Method                  | Key Features & Tricks                          | Training Style         | Functional Modules                                     | Strengths                        | Weaknesses                                       |\n",
    "# | ------------------------ | ----- | ---------------------------- | ---------------------------------------------- | ---------------------- | ----------------------------------------------------------------------------------------- | ------------------------------------------------ |\n",
    "# | **CART** (Decision Tree) | 1986  | Greedy splits                | Gini impurity / MSE; max depth; pruning        | Recursive partitioning | - Split criteria                                       | Simple, interpretable            | High variance, overfitting                       |\n",
    "# |                          |       |                              |                                                |                        | - Tree structure                                       |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Pruning                                              |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Impurity computation                                 |                                  |                                                  |\n",
    "# | **Bagging**              | 1996  | Bootstrapped trees           | Multiple trees on random samples               | Parallel training      | - Bootstrap sampler                                    | Reduces variance                 | Still sensitive to overfitting on noisy features |\n",
    "# |                          |       |                              |                                                |                        | - Aggregation (voting/averaging)                       |                                  |                                                  |\n",
    "# | **Random Forest**        | 2001  | Bagging + feature randomness | Random feature subset at each split            | Parallel training      | - Bootstrap sampler                                    | Robust, handles high dimensions  | Slow for large datasets                          |\n",
    "# |                          |       |                              |                                                |                        | - Feature subspace sampler                             |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Tree ensemble                                        |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Majority voting                                      |                                  |                                                  |\n",
    "# | **ExtraTrees**           | 2006  | Fully randomized trees       | Random feature **and** threshold selection     | Parallel training      | - Random threshold selector                            | Very fast, low variance          | Slightly higher bias                             |\n",
    "# |                          |       |                              |                                                |                        | - Feature subspace                                     |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Ensemble aggregator                                  |                                  |                                                  |\n",
    "# | **XGBoost**              | 2014  | Gradient Boosting            | Regularization, shrinkage, weighted splits     | Sequential boosting    | - Gradient calculator                                  | High accuracy, scalable          | Sensitive to hyperparams                         |\n",
    "# |                          |       |                              |                                                |                        | - Loss function                                        |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Tree pruner                                          |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Column block optimization                            |                                  |                                                  |\n",
    "# | **LightGBM**             | 2017  | Gradient Boosting            | Leaf-wise growth, histogram bins               | Sequential boosting    | - Histogram binning                                    | Fast, efficient memory use       | Overfits small data if not regularized           |\n",
    "# |                          |       |                              |                                                |                        | - Leaf-wise tree builder                               |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - GPU training                                         |                                  |                                                  |\n",
    "# | **CatBoost**             | 2017  | Ordered Boosting             | Categorical encoding (ordered target encoding) | Sequential boosting    | - Ordered target encoder                               | Best with categorical features   | Slower on numeric-only datasets                  |\n",
    "# |                          |       |                              |                                                |                        | - Symmetric trees                                      |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Bayesian averaging                                   |                                  |                                                  |\n",
    "# | **gcForest**             | 2017  | Layered Forests              | Deep cascade of forests, auto ensemble         | Layer-wise cascading   | - Multi-grain scanning                                 | Handles small data well          | Complex to tune and understand                   |\n",
    "# |                          |       |                              |                                                |                        | - Cascaded forests                                     |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Auto model selection                                 |                                  |                                                  |\n",
    "# | **Neural Forests**       | 2020s | Soft/diff. splits            | Differentiable nodes, hybrid with neural nets  | Backpropagation + SGD  | - Soft split function (sigmoid)                        | Can be trained end-to-end        | Less interpretable, newer technique              |\n",
    "# |                          |       |                              |                                                |                        | - Neural layers                                        |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Loss-based gradient optimization                     |                                  |                                                  |\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# following is the model selection process for \"Classic Pure Time Series Models\":\n",
    "# =============================================================================\n",
    "# Example: Uniqueness is stationary, has a long decaying autocorrelation(cutoff at lag >10) and partial autocorrelation(cutoff at lag 2).\n",
    "\n",
    "# Start\n",
    "#  |\n",
    "#  |-- Is the series stationary? (ADF test, KPSS test, plot)\n",
    "#  |     | ADF (Augmented Dickey-Fuller) test\n",
    "#  |     | KPSS (Kwiatkowski–Phillips–Schmidt–Shin) test\n",
    "#  |     |-- No --> Apply differencing (log/first/second diff)\n",
    "#  |     |            |\n",
    "#  |     |            |-- Recheck stationarity\n",
    "#  |     |                   |\n",
    "#  |     |                   |-- Still non-stationary? --> Consider advanced models (e.g., trend modeling, transformations)\n",
    "#  |     |\n",
    "#  |     |-- Yes\n",
    "#  |\n",
    "#  |-- Examine ACF and PACF plots (on stationary series)\n",
    "#  |     |\n",
    "#  |     |-- ACF cuts off at lag q, PACF tails off --> MA(q)\n",
    "#  |     |\n",
    "#  |     |-- PACF cuts off at lag p, ACF tails off --> AR(p)\n",
    "#  |     |\n",
    "#  |     |-- Both ACF and PACF tail off --> ARMA(p, q)\n",
    "#  |     |\n",
    "#  |     |-- ACF and PACF have pattern after differencing --> ARIMA(p, d, q)\n",
    "#  |\n",
    "#  |-- For Seasonal Patterns? (ACF spikes at seasonal lags like 12, 24)\n",
    "#  |     |\n",
    "#  |     |-- Yes --> Use SARIMA(p,d,q)(P,D,Q,s)\n",
    "#  |     |          |\n",
    "#  |     |          |-- Use seasonal ACF/PACF for seasonal P and Q\n",
    "#  |\n",
    "#  |-- Fit candidate models (e.g., ARIMA, MA, AR, SARIMA)\n",
    "#  |     |\n",
    "#  |     |-- Evaluate using AIC, BIC\n",
    "#  |     |     |\n",
    "#  |     |     |-- Select models with lowest AIC/BIC\n",
    "#  |\n",
    "#  |-- Perform residual diagnostics on selected models\n",
    "#  |     |\n",
    "#  |     |-- Are residuals uncorrelated? (Ljung-Box test)\n",
    "#  |     |-- Are residuals homoscedastic and normal?\n",
    "#  |     |\n",
    "#  |     |-- No --> Model misspecification → Refine (adjust p/q, add seasonal terms)\n",
    "#  |     |\n",
    "#  |     |-- Yes\n",
    "#  |\n",
    "#  |-- (Optional) Compare forecast accuracy on validation data\n",
    "#  |     |\n",
    "#  |     |-- Use RMSE, MAE, MAPE\n",
    "#  |\n",
    "#  |-- Are multiple models close in performance?\n",
    "#  |     |\n",
    "#  |     |-- Yes --> Choose simpler (lower-order) model\n",
    "#  |     |\n",
    "#  |     |-- No --> Choose best-performing model\n",
    "#  |\n",
    "#  ✅ Final Model Selected\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2bec496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.956024e+04</td>\n",
       "      <td>1026.461275</td>\n",
       "      <td>16500.500000</td>\n",
       "      <td>18946.750000</td>\n",
       "      <td>19636.500000</td>\n",
       "      <td>20201.687500</td>\n",
       "      <td>21521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.957602e+04</td>\n",
       "      <td>1021.795364</td>\n",
       "      <td>16534.500000</td>\n",
       "      <td>18965.062500</td>\n",
       "      <td>19648.500000</td>\n",
       "      <td>20215.125000</td>\n",
       "      <td>21529.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.954397e+04</td>\n",
       "      <td>1030.919129</td>\n",
       "      <td>16452.500000</td>\n",
       "      <td>18928.000000</td>\n",
       "      <td>19624.000000</td>\n",
       "      <td>20185.062500</td>\n",
       "      <td>21513.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.956023e+04</td>\n",
       "      <td>1026.465353</td>\n",
       "      <td>16499.500000</td>\n",
       "      <td>18946.437500</td>\n",
       "      <td>19636.250000</td>\n",
       "      <td>20202.312500</td>\n",
       "      <td>21521.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>4.428075e-01</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_ret</th>\n",
       "      <td>20159.0</td>\n",
       "      <td>-2.444558e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom1</th>\n",
       "      <td>20159.0</td>\n",
       "      <td>5.933983e-07</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>-0.045618</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.025041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom2</th>\n",
       "      <td>20158.0</td>\n",
       "      <td>1.188938e-06</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>-0.048490</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.030751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom3</th>\n",
       "      <td>20157.0</td>\n",
       "      <td>1.803692e-06</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>-0.052876</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.040459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom4</th>\n",
       "      <td>20156.0</td>\n",
       "      <td>2.418886e-06</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>-0.053822</td>\n",
       "      <td>-0.001254</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom5</th>\n",
       "      <td>20155.0</td>\n",
       "      <td>3.026053e-06</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>-0.052905</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.048939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>1.080068e-03</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.007921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_1</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.625446e-02</td>\n",
       "      <td>0.139996</td>\n",
       "      <td>-0.634113</td>\n",
       "      <td>-0.118942</td>\n",
       "      <td>-0.025649</td>\n",
       "      <td>0.069316</td>\n",
       "      <td>0.475915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_2</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-1.499542e-02</td>\n",
       "      <td>0.138863</td>\n",
       "      <td>-0.447951</td>\n",
       "      <td>-0.112155</td>\n",
       "      <td>-0.019590</td>\n",
       "      <td>0.082092</td>\n",
       "      <td>0.545940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_3</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.895765e-02</td>\n",
       "      <td>0.139141</td>\n",
       "      <td>-0.497556</td>\n",
       "      <td>-0.123967</td>\n",
       "      <td>-0.029113</td>\n",
       "      <td>0.066792</td>\n",
       "      <td>0.467896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_4</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.548542e-02</td>\n",
       "      <td>0.147383</td>\n",
       "      <td>-0.495312</td>\n",
       "      <td>-0.129528</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>0.080143</td>\n",
       "      <td>0.506904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_5</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.338549e-02</td>\n",
       "      <td>0.147182</td>\n",
       "      <td>-0.558187</td>\n",
       "      <td>-0.122946</td>\n",
       "      <td>-0.023834</td>\n",
       "      <td>0.077938</td>\n",
       "      <td>0.472844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t1</th>\n",
       "      <td>20158.0</td>\n",
       "      <td>-2.560565e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t2</th>\n",
       "      <td>20157.0</td>\n",
       "      <td>-2.543307e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t3</th>\n",
       "      <td>20156.0</td>\n",
       "      <td>-2.670947e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t4</th>\n",
       "      <td>20155.0</td>\n",
       "      <td>-3.169885e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t5</th>\n",
       "      <td>20154.0</td>\n",
       "      <td>-3.210667e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fast_mavg</th>\n",
       "      <td>20154.0</td>\n",
       "      <td>1.955967e+04</td>\n",
       "      <td>1025.773046</td>\n",
       "      <td>16533.821429</td>\n",
       "      <td>18943.187500</td>\n",
       "      <td>19637.803571</td>\n",
       "      <td>20198.312500</td>\n",
       "      <td>21508.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_mavg</th>\n",
       "      <td>20146.0</td>\n",
       "      <td>1.955893e+04</td>\n",
       "      <td>1024.922277</td>\n",
       "      <td>16565.583333</td>\n",
       "      <td>18942.870833</td>\n",
       "      <td>19639.958333</td>\n",
       "      <td>20197.787500</td>\n",
       "      <td>21502.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atr</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>3.206390e+01</td>\n",
       "      <td>16.285737</td>\n",
       "      <td>11.114555</td>\n",
       "      <td>22.750606</td>\n",
       "      <td>27.546002</td>\n",
       "      <td>35.127528</td>\n",
       "      <td>204.381303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean          std           min           25%  \\\n",
       "open        20160.0  1.956024e+04  1026.461275  16500.500000  18946.750000   \n",
       "high        20160.0  1.957602e+04  1021.795364  16534.500000  18965.062500   \n",
       "low         20160.0  1.954397e+04  1030.919129  16452.500000  18928.000000   \n",
       "close       20160.0  1.956023e+04  1026.465353  16499.500000  18946.437500   \n",
       "label       20160.0  4.428075e-01     0.496731      0.000000      0.000000   \n",
       "log_ret     20159.0 -2.444558e-07     0.001295     -0.046691     -0.000659   \n",
       "mom1        20159.0  5.933983e-07     0.001294     -0.045618     -0.000659   \n",
       "mom2        20158.0  1.188938e-06     0.001830     -0.048490     -0.000909   \n",
       "mom3        20157.0  1.803692e-06     0.002260     -0.052876     -0.001099   \n",
       "mom4        20156.0  2.418886e-06     0.002603     -0.053822     -0.001254   \n",
       "mom5        20155.0  3.026053e-06     0.002912     -0.052905     -0.001446   \n",
       "volatility  20110.0  1.080068e-03     0.000716      0.000309      0.000710   \n",
       "autocorr_1  20110.0 -2.625446e-02     0.139996     -0.634113     -0.118942   \n",
       "autocorr_2  20110.0 -1.499542e-02     0.138863     -0.447951     -0.112155   \n",
       "autocorr_3  20110.0 -2.895765e-02     0.139141     -0.497556     -0.123967   \n",
       "autocorr_4  20110.0 -2.548542e-02     0.147383     -0.495312     -0.129528   \n",
       "autocorr_5  20110.0 -2.338549e-02     0.147182     -0.558187     -0.122946   \n",
       "log_t1      20158.0 -2.560565e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t2      20157.0 -2.543307e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t3      20156.0 -2.670947e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t4      20155.0 -3.169885e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t5      20154.0 -3.210667e-07     0.001295     -0.046691     -0.000659   \n",
       "fast_mavg   20154.0  1.955967e+04  1025.773046  16533.821429  18943.187500   \n",
       "slow_mavg   20146.0  1.955893e+04  1024.922277  16565.583333  18942.870833   \n",
       "atr         20160.0  3.206390e+01    16.285737     11.114555     22.750606   \n",
       "\n",
       "                     50%           75%           max  \n",
       "open        19636.500000  20201.687500  21521.000000  \n",
       "high        19648.500000  20215.125000  21529.750000  \n",
       "low         19624.000000  20185.062500  21513.000000  \n",
       "close       19636.250000  20202.312500  21521.750000  \n",
       "label           0.000000      1.000000      1.000000  \n",
       "log_ret         0.000000      0.000649      0.024732  \n",
       "mom1            0.000000      0.000649      0.025041  \n",
       "mom2            0.000000      0.000889      0.030751  \n",
       "mom3            0.000000      0.001070      0.040459  \n",
       "mom4           -0.000013      0.001221      0.047852  \n",
       "mom5           -0.000025      0.001383      0.048939  \n",
       "volatility      0.000878      0.001141      0.007921  \n",
       "autocorr_1     -0.025649      0.069316      0.475915  \n",
       "autocorr_2     -0.019590      0.082092      0.545940  \n",
       "autocorr_3     -0.029113      0.066792      0.467896  \n",
       "autocorr_4     -0.028310      0.080143      0.506904  \n",
       "autocorr_5     -0.023834      0.077938      0.472844  \n",
       "log_t1          0.000000      0.000649      0.024732  \n",
       "log_t2          0.000000      0.000649      0.024732  \n",
       "log_t3          0.000000      0.000649      0.024732  \n",
       "log_t4          0.000000      0.000649      0.024732  \n",
       "log_t5          0.000000      0.000649      0.024732  \n",
       "fast_mavg   19637.803571  20198.312500  21508.035714  \n",
       "slow_mavg   19639.958333  20197.787500  21502.150000  \n",
       "atr            27.546002     35.127528    204.381303  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label.parquet'))\n",
    "raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label_trend_1to8.parquet'))\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label_trend_3to24.parquet'))\n",
    "raw_data = raw_data[-(60*24*14):].copy()\n",
    "\n",
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close'] / raw_data['close'].shift(1))\n",
    "\n",
    "# Momentum\n",
    "for i in range(1, 6):\n",
    "    raw_data[f'mom{i}'] = raw_data['close'].pct_change(periods=i)\n",
    "\n",
    "# Volatility\n",
    "window_stdev = 50\n",
    "raw_data['volatility'] = raw_data['log_ret'].rolling(window=window_stdev, min_periods=window_stdev).std()\n",
    "\n",
    "# Serial Correlation\n",
    "window_autocorr = 50\n",
    "for lag in range(1, 6):\n",
    "    raw_data[f'autocorr_{lag}'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr).apply(\n",
    "        lambda x: x.autocorr(lag=lag) if x.notna().sum() > lag else np.nan,\n",
    "        raw=False\n",
    "    )\n",
    "\n",
    "# Lagged log returns\n",
    "for i in range(1, 6):\n",
    "    raw_data[f'log_t{i}'] = raw_data['log_ret'].shift(i)\n",
    "\n",
    "# Moving averages\n",
    "fast_window = 7\n",
    "slow_window = 15\n",
    "raw_data['fast_mavg'] = raw_data['close'].rolling(window=fast_window, min_periods=fast_window).mean()\n",
    "raw_data['slow_mavg'] = raw_data['close'].rolling(window=slow_window, min_periods=slow_window).mean()\n",
    "\n",
    "# ATR\n",
    "high_low = raw_data['high'] - raw_data['low']\n",
    "high_close = (raw_data['high'] - raw_data['close'].shift()).abs()\n",
    "low_close = (raw_data['low'] - raw_data['close'].shift()).abs()\n",
    "tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "raw_data['atr'] = tr.ewm(span=12, adjust=False).mean()\n",
    "\n",
    "# raw_data.info(verbose=True, memory_usage='deep')\n",
    "raw_data.describe().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b81c93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    11233\n",
      "1     8927\n",
      "Name: count, dtype: int64\n",
      "Out-of-bag Accuracy (OOB Score): 0.567956\n"
     ]
    }
   ],
   "source": [
    "# df = raw_data[-(60*24*7):].copy()\n",
    "df = raw_data.copy()\n",
    "\n",
    "# H_PER_D = 23                                # trade hours per day\n",
    "# P_PER_B = 5                                 # equivalent bar period\n",
    "# EMA_VOL_SPAN = int(60/P_PER_B * H_PER_D)    # span for EMA volatility (daily)\n",
    "# CUSUM_FACTOR = 0.6                          # multiplier for CUSUM threshold\n",
    "# \n",
    "# # --- VOLATILITY AND FILTER ---\n",
    "# label = df[['label']].copy()\n",
    "# label['ref'] = np.log((df['close'] / df['close'].iloc[0]).fillna(1))\n",
    "# # EMA-based volatility on log returns\n",
    "# label['return'] = df['close'].pct_change().fillna(0)\n",
    "# # daily vol (volume/run bars has more constant volatility for their homoscedasticity)\n",
    "# label['pos_return'] = label['return'].where(label['return'] > 0, 0.0001)\n",
    "# label['neg_return'] = label['return'].where(label['return'] < 0, 0.0001)\n",
    "# label['pos_vol'] = label['pos_return'].ewm(span=EMA_VOL_SPAN, adjust=False).std().replace(0, np.nan).ffill().bfill()\n",
    "# label['neg_vol'] = label['neg_return'].ewm(span=EMA_VOL_SPAN, adjust=False).std().replace(0, np.nan).ffill().bfill()\n",
    "# \n",
    "# # CUSUM to mark breakout events and direction\n",
    "# s_pos, s_neg = 0.0, 0.0\n",
    "# label['event'] = 0.0 # np.nan\n",
    "# for i in range(1, len(label)):\n",
    "#     # note that for time i, the label/prediction is calculated after all info of that time is known\n",
    "#     pos_threshold = label['pos_vol'].iloc[i] * CUSUM_FACTOR\n",
    "#     neg_threshold = label['neg_vol'].iloc[i] * CUSUM_FACTOR\n",
    "#     diff = label['return'].iloc[i]\n",
    "#     s_pos = max(0, s_pos + diff)\n",
    "#     s_neg = min(0, s_neg + diff)\n",
    "#     index = label.index[i]\n",
    "#     if s_pos > pos_threshold:\n",
    "#         label.loc[index, 'event'] = 1\n",
    "#         s_pos = 0.0\n",
    "#     elif s_neg < -neg_threshold:\n",
    "#         label.loc[index, 'event'] = -1\n",
    "#         s_neg = 0.0\n",
    "# \n",
    "# index = label.index[label['event'] != 0]\n",
    "# \n",
    "# print(df.shape)\n",
    "# df.drop(columns=['open', 'high', 'low', 'close', 'uniqueness'], axis=1, inplace=True)\n",
    "# df = df.loc[index]\n",
    "# print(df.shape)\n",
    "\n",
    "# | σ    | Cumulative Probability | Approx. % within ±σ range          |\n",
    "# |------|------------------------|------------------------------------|\n",
    "# | 0.1  | 0.0797                 | ~7.97% within ±0.1σ                |\n",
    "# | 0.2  | 0.1587                 | ~15.87% within ±0.2σ               |\n",
    "# | 0.3  | 0.2266                 | ~22.66% within ±0.3σ               |\n",
    "# | 0.4  | 0.3108                 | ~31.08% within ±0.4σ               |\n",
    "# | 0.5  | 0.3829                 | ~38.29% within ±0.5σ               |\n",
    "# | 1.0  | 0.6827                 | ~68.27% within ±1σ (1-sigma rule)  |\n",
    "# | 1.5  | 0.8664                 | ~86.64% within ±1.5σ               |\n",
    "# | 2.0  | 0.9545                 | ~95.45% within ±2σ (2-sigma rule)  |\n",
    "# | 2.5  | 0.9876                 | ~98.76% within ±2.5σ               |\n",
    "\n",
    "label = df['label']\n",
    "sigma = label.std()*0.5 # assume normal distribution (actually scaled version)\n",
    "def classify(x):\n",
    "    if x < -sigma:\n",
    "        return -1\n",
    "    elif x > sigma:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "y = df['label'].apply(classify)\n",
    "X = df.drop(columns=['label'], axis=1)\n",
    "\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split data into training, validation and test sets\n",
    "X_training_test = X\n",
    "y_training_test = y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training_test, y_training_test, test_size=0.2, shuffle=False)\n",
    "\n",
    "n_estimator = 100\n",
    "depth = 5\n",
    "c_random_state = 42\n",
    "\n",
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator, oob_score=True,\n",
    "                            criterion='entropy', random_state=c_random_state)\n",
    "rf.fit(X_train, y_train.values.ravel())\n",
    "print(\"Out-of-bag Accuracy (OOB Score): {:.6f}\".format(rf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad7f1da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=1, shuffle=True)\n",
      "[0.55505952 0.54910714 0.56646825 0.55952381 0.5625    ]\n",
      "Mean KFold accuracy: 0.558532\n"
     ]
    }
   ],
   "source": [
    "# K-fold\n",
    "no_of_folds = 5\n",
    "kfold = KFold(shuffle=True, random_state=1, n_splits=no_of_folds)\n",
    "print(kfold)\n",
    "\n",
    "accuracy_array = np.zeros(no_of_folds)\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X_training_test.values):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf.fit(X_train, np.array(y_train).ravel())\n",
    "\n",
    "    y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy_array[i] = accuracy_score(y_test, y_pred)\n",
    "    i += 1\n",
    "    # print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(accuracy_array)\n",
    "print(\"Mean KFold accuracy: {:.6f}\".format(np.mean(accuracy_array)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard models like RandomForest/Boosting try to optimize overall accuracy, which is practically impossible for low SNR environment\n",
    "# even worse, forcefully doing so may result in terrible overfitting (e.g. boosting require high quality base models to yield real losses(not just noise), to be fitted again later)\n",
    "\n",
    "# we introduce Confidence-Based Ensemble Models for environments with:\n",
    "# - signal is sparse, noise is constantly present\n",
    "# - false positives are costly\n",
    "# - rare combinations matter\n",
    "\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Model / Approach       | Key Idea / Mechanism                       | Confidence Estimation        | Output Selection / Aggregation| Strengths                      | Weaknesses                    | Paper / Origin      |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Deep Ensembles         | Train multiple NNs independently           | Predictive variance across   | Mean prediction; variance     | Captures epistemic uncertainty;| Expensive to train and store  | Lakshminarayanan    |\n",
    "# |                        | with different inits and data shuffles     | models with different inits  | indicates uncertainty         | strong calibration             | multiple large models         | et al., 2017        |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | MC-Dropout             | Use dropout at test time to sample         | Variance across dropout      | Mean and variance of          | Lightweight Bayesian inference;| Dropout tuning is critical;   | Gal & Ghahramani,   |\n",
    "# |                        | outputs (Bayesian approximation)           | outputs from same network    | predictions                   | works on existing models       | not always reliable           | 2016                |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Bootstrap Ensembles    | Train NNs on bootstrapped datasets         | Output spread across         | Voting or averaging +         | Handles label noise well;      | Can underperform in data-     | Classic bagging     |\n",
    "# |                        | to capture diverse noise patterns          | independently trained models | confidence threshold          | diverse feature exploration    | scarce regimes                | with NNs            |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | SelectiveNet           | Jointly learns to predict and abstain      | Trainable confidence         | Predict only when             | High precision when selective; | Needs abstention-aware        | Geifman & El-Yaniv, |\n",
    "# |                        | using an auxiliary confidence head         | head output                  | confidence exceeds threshold  | end-to-end trainable           | loss; may skip hard cases     | 2019                |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Conf.-Aware KD         | Student learns from confident              | Entropy or margin of         | Filter teacher targets        | Robust to noisy labels;        | Requires strong teacher;      | Hinton-style KD     |\n",
    "# |                        | teacher predictions in KD setup            | teacher predictions          | by confidence                 | inherits reliable signal       | depends on confident signal   | + confidence gating |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | DeepGAM                | Ensemble with soft feature-gating          | Softmax gate activation      | Weighted expert output        | Interpretable gating;          | Gates may confuse;            | Chang et al.,       |\n",
    "# |                        | where each expert activates conditionally  | controls expert firing       | by gating network             | local specialization           | model may not abstain         | 2021 (ICML)         |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Bayesian NNs (BNNs)    | Treat weights as distributions             | Posterior variance from      | Mean prediction + credible    | Captures epistemic +           | Hard to scale; variational    | Blundell et al.,    |\n",
    "# |                        | and output as expectation                  | sampling posterior weights   | interval for confidence       | aleatoric uncertainty          | methods often poor approx.    | 2015                |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Ensemble Temp. Scaling | Calibrate softmax with temperature         | Softmax confidence           | Apply confidence thresholds   | Improves probabilistic         | Doesn’t affect predictions;   | Guo et al.,         |\n",
    "# |                        | post-training                              | after scaling                | post calibration              | calibration of models          | only recalibrates scores      | 2017 (NIPS)         |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | DUQ                    | Distance to class prototypes               | Distance from embedding      | Predict only if within        | Simple and fast;               | Limited to clear class        | van Amersfoort      |\n",
    "# |                        | gives predictive confidence                | to known class centroids     | decision boundary             | no sampling required           | boundaries; low flexibility   | et al., 2020        |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Focal Loss Ensembles   | Use focal loss to prioritize               | Internal confidence          | Aggregate only when           | Improves recall on rare        | Sensitive to tuning of        | Lin et al.,         |\n",
    "# |                        | hard/rare samples in ensemble              | based on loss weighting      | model is confident            | events; reduces overfitting    | focal loss params             | 2017 (RetinaNet)    |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Trust Score Ensemble   | Post-hoc distance ratio                    | Ratio of distances to        | Select models where           | Interpretable; simple          | Needs external scoring        |                     |\n",
    "# |                        | for estimating prediction trust            | nearest labeled samples      | trust score exceeds threshold | and model-agnostic             | set; limited to clean regions |                     |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
