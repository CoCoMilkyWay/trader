{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# following is the model selection process for \"Classic Pure Time Series Models\":\n",
    "# =============================================================================\n",
    "# Example: Uniqueness is stationary, has a long decaying autocorrelation(cutoff at lag >10) and partial autocorrelation(cutoff at lag 2).\n",
    "\n",
    "# Start\n",
    "#  |\n",
    "#  |-- Is the series stationary? (ADF test, KPSS test, plot)\n",
    "#  |     | ADF (Augmented Dickey-Fuller) test\n",
    "#  |     | KPSS (Kwiatkowski–Phillips–Schmidt–Shin) test\n",
    "#  |     |-- No --> Apply differencing (log/first/second diff)\n",
    "#  |     |            |\n",
    "#  |     |            |-- Recheck stationarity\n",
    "#  |     |                   |\n",
    "#  |     |                   |-- Still non-stationary? --> Consider advanced models (e.g., trend modeling, transformations)\n",
    "#  |     |\n",
    "#  |     |-- Yes\n",
    "#  |\n",
    "#  |-- Examine ACF and PACF plots (on stationary series)\n",
    "#  |     |\n",
    "#  |     |-- ACF cuts off at lag q, PACF tails off --> MA(q)\n",
    "#  |     |\n",
    "#  |     |-- PACF cuts off at lag p, ACF tails off --> AR(p)\n",
    "#  |     |\n",
    "#  |     |-- Both ACF and PACF tail off --> ARMA(p, q)\n",
    "#  |     |\n",
    "#  |     |-- ACF and PACF have pattern after differencing --> ARIMA(p, d, q)\n",
    "#  |\n",
    "#  |-- For Seasonal Patterns? (ACF spikes at seasonal lags like 12, 24)\n",
    "#  |     |\n",
    "#  |     |-- Yes --> Use SARIMA(p,d,q)(P,D,Q,s)\n",
    "#  |     |          |\n",
    "#  |     |          |-- Use seasonal ACF/PACF for seasonal P and Q\n",
    "#  |\n",
    "#  |-- Fit candidate models (e.g., ARIMA, MA, AR, SARIMA)\n",
    "#  |     |\n",
    "#  |     |-- Evaluate using AIC, BIC\n",
    "#  |     |     |\n",
    "#  |     |     |-- Select models with lowest AIC/BIC\n",
    "#  |\n",
    "#  |-- Perform residual diagnostics on selected models\n",
    "#  |     |\n",
    "#  |     |-- Are residuals uncorrelated? (Ljung-Box test)\n",
    "#  |     |-- Are residuals homoscedastic and normal?\n",
    "#  |     |\n",
    "#  |     |-- No --> Model misspecification → Refine (adjust p/q, add seasonal terms)\n",
    "#  |     |\n",
    "#  |     |-- Yes\n",
    "#  |\n",
    "#  |-- (Optional) Compare forecast accuracy on validation data\n",
    "#  |     |\n",
    "#  |     |-- Use RMSE, MAE, MAPE\n",
    "#  |\n",
    "#  |-- Are multiple models close in performance?\n",
    "#  |     |\n",
    "#  |     |-- Yes --> Choose simpler (lower-order) model\n",
    "#  |     |\n",
    "#  |     |-- No --> Choose best-performing model\n",
    "#  |\n",
    "#  ✅ Final Model Selected\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a20dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# We consider family of random forest as baseline \"Feature-Based Time Series (ML)Models\" here:\n",
    "# =============================================================================\n",
    "# | Model                    | Year  | Core Method                  | Key Features & Tricks                          | Training Style         | Functional Modules                                     | Strengths                        | Weaknesses                                       |\n",
    "# | ------------------------ | ----- | ---------------------------- | ---------------------------------------------- | ---------------------- | ----------------------------------------------------------------------------------------- | ------------------------------------------------ |\n",
    "# | **CART** (Decision Tree) | 1986  | Greedy splits                | Gini impurity / MSE; max depth; pruning        | Recursive partitioning | - Split criteria                                       | Simple, interpretable            | High variance, overfitting                       |\n",
    "# |                          |       |                              |                                                |                        | - Tree structure                                       |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Pruning                                              |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Impurity computation                                 |                                  |                                                  |\n",
    "# | **Bagging**              | 1996  | Bootstrapped trees           | Multiple trees on random samples               | Parallel training      | - Bootstrap sampler                                    | Reduces variance                 | Still sensitive to overfitting on noisy features |\n",
    "# |                          |       |                              |                                                |                        | - Aggregation (voting/averaging)                       |                                  |                                                  |\n",
    "# | **Random Forest**        | 2001  | Bagging + feature randomness | Random feature subset at each split            | Parallel training      | - Bootstrap sampler                                    | Robust, handles high dimensions  | Slow for large datasets                          |\n",
    "# |                          |       |                              |                                                |                        | - Feature subspace sampler                             |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Tree ensemble                                        |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Majority voting                                      |                                  |                                                  |\n",
    "# | **ExtraTrees**           | 2006  | Fully randomized trees       | Random feature **and** threshold selection     | Parallel training      | - Random threshold selector                            | Very fast, low variance          | Slightly higher bias                             |\n",
    "# |                          |       |                              |                                                |                        | - Feature subspace                                     |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Ensemble aggregator                                  |                                  |                                                  |\n",
    "# | **XGBoost**              | 2014  | Gradient Boosting            | Regularization, shrinkage, weighted splits     | Sequential boosting    | - Gradient calculator                                  | High accuracy, scalable          | Sensitive to hyperparams                         |\n",
    "# |                          |       |                              |                                                |                        | - Loss function                                        |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Tree pruner                                          |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Column block optimization                            |                                  |                                                  |\n",
    "# | **LightGBM**             | 2017  | Gradient Boosting            | Leaf-wise growth, histogram bins               | Sequential boosting    | - Histogram binning                                    | Fast, efficient memory use       | Overfits small data if not regularized           |\n",
    "# |                          |       |                              |                                                |                        | - Leaf-wise tree builder                               |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - GPU training                                         |                                  |                                                  |\n",
    "# | **CatBoost**             | 2017  | Ordered Boosting             | Categorical encoding (ordered target encoding) | Sequential boosting    | - Ordered target encoder                               | Best with categorical features   | Slower on numeric-only datasets                  |\n",
    "# |                          |       |                              |                                                |                        | - Symmetric trees                                      |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Bayesian averaging                                   |                                  |                                                  |\n",
    "# | **gcForest**             | 2017  | Layered Forests              | Deep cascade of forests, auto ensemble         | Layer-wise cascading   | - Multi-grain scanning                                 | Handles small data well          | Complex to tune and understand                   |\n",
    "# |                          |       |                              |                                                |                        | - Cascaded forests                                     |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Auto model selection                                 |                                  |                                                  |\n",
    "# | **Neural Forests**       | 2020s | Soft/diff. splits            | Differentiable nodes, hybrid with neural nets  | Backpropagation + SGD  | - Soft split function (sigmoid)                        | Can be trained end-to-end        | Less interpretable, newer technique              |\n",
    "# |                          |       |                              |                                                |                        | - Neural layers                                        |                                  |                                                  |\n",
    "# |                          |       |                              |                                                |                        | - Loss-based gradient optimization                     |                                  |                                                  |\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long term models (macro/micro(fundamentals)-economics indicators):\n",
    "#   - can have good overall accuracy\n",
    "#   - features are relatively strong, high covariance to predictable target\n",
    "# models: ridge, lasso, penalized regression models(elastic nets), classic ensembles like RandomForest(XGBoost) type\n",
    "\n",
    "# Short term models (market micro-structures):\n",
    "#   - signal is sparse, noise is constantly present\n",
    "#   - prioritize Precision/Recall/F1 over overall accuracy\n",
    "#   - clean signals(trading opportunities) only appear after certain feature combination with certain values\n",
    "# models: most simply doesn't work\n",
    "\n",
    "# there are 2 paths for ML4F:\n",
    "#   - Machine Learning\n",
    "#     - model learns combinations from thousands of weak features\n",
    "#     - Model quality is key, most models can't do this well(this even include bootstrap ensembles with boosting like XGBoost, LightGBM, CatBoost)\n",
    "#     - performance decay is slower, because this is a more general approach, doesn't require large number of high-quality alphas(which decay fast)\n",
    "#     - once decay, simply refit and you will have a good model again\n",
    "#     - work well with mined alphas\n",
    "#   - Alpha Mining:\n",
    "#     - model relies on high quality mined alphas, which will have good covariance with predicting target\n",
    "#     - simpler models can work\n",
    "#     - need very sophisticated alpha mining algorithm(maybe with RL), very hard(technically) to eliminate overfitting\n",
    "#     - performance decay fast\n",
    "\n",
    "# standard models like RandomForest/Boosting/NN/ResNet is unlikely to work well on sparse-signals/weak-features:\n",
    "#     - these are all MLE-alike models(trees are discriminative, NN can be either discriminative or generative(VAE/GAN)), when you cannot fit a good model, you should probably lower expectation only trade when opportunities are more certain\n",
    "#     - it tries to optimize overall accuracy, which is practically impossible for low SNR environment\n",
    "#     - these model make inherent assumptions like: target classes are continuous in feature space (thus the greedy split)\n",
    "#     - its relatively simple features selection will include the weak(high noise) features to grow trees, which will not have good performance\n",
    "#     - even worse, boosting/ResNet requires high quality fitted base models(which will not happen/converge) to yield real/meaningful loss values, to be fitted again later\n",
    "#     - forcefully fitting under low SNR with weak features will result in terrible overfitting (both in based model, and more so in residue models)\n",
    "\n",
    "# we introduce Confidence-Based Ensemble Models for market micro-structure models:\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Model / Approach       | Key Idea / Mechanism                       | Confidence Estimation        | Output Selection / Aggregation| Strengths                      | Weaknesses                    | Paper / Origin      |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Deep Ensembles         | Train multiple NNs independently           | Predictive variance across   | Mean prediction; variance     | Captures epistemic uncertainty;| Expensive to train and store  | Lakshminarayanan    |\n",
    "# |                        | with different inits and data shuffles     | models with different inits  | indicates uncertainty         | strong calibration             | multiple large models         | et al., 2017        |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | MC-Dropout             | Use dropout at test time to sample         | Variance across dropout      | Mean and variance of          | Lightweight Bayesian inference;| Dropout tuning is critical;   | Gal & Ghahramani,   |\n",
    "# |                        | outputs (Bayesian approximation)           | outputs from same network    | predictions                   | works on existing models       | not always reliable           | 2016                |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Bootstrap Ensembles    | Train NNs on bootstrapped datasets         | Output spread across         | Voting or averaging +         | Handles label noise well;      | Can underperform in data-     | Classic bagging     |\n",
    "# |                        | to capture diverse noise patterns          | independently trained models | confidence threshold          | diverse feature exploration    | scarce regimes                | with NNs            |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | SelectiveNet           | Jointly learns to predict and abstain      | Trainable confidence         | Predict only when             | High precision when selective; | Needs abstention-aware        | Geifman & El-Yaniv, |\n",
    "# |                        | using an auxiliary confidence head         | head output                  | confidence exceeds threshold  | end-to-end trainable           | loss; may skip hard cases     | 2019                |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Conf.-Aware KD         | Student learns from confident              | Entropy or margin of         | Filter teacher targets        | Robust to noisy labels;        | Requires strong teacher;      | Hinton-style KD     |\n",
    "# |                        | teacher predictions in KD setup            | teacher predictions          | by confidence                 | inherits reliable signal       | depends on confident signal   | + confidence gating |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | DeepGAM                | Ensemble with soft feature-gating          | Softmax gate activation      | Weighted expert output        | Interpretable gating;          | Gates may confuse;            | Chang et al.,       |\n",
    "# |                        | where each expert activates conditionally  | controls expert firing       | by gating network             | local specialization           | model may not abstain         | 2021 (ICML)         |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Bayesian NNs (BNNs)    | Treat weights as distributions             | Posterior variance from      | Mean prediction + credible    | Captures epistemic +           | Hard to scale; variational    | Blundell et al.,    |\n",
    "# |                        | and output as expectation                  | sampling posterior weights   | interval for confidence       | aleatoric uncertainty          | methods often poor approx.    | 2015                |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Ensemble Temp. Scaling | Calibrate softmax with temperature         | Softmax confidence           | Apply confidence thresholds   | Improves probabilistic         | Doesn’t affect predictions;   | Guo et al.,         |\n",
    "# |                        | post-training                              | after scaling                | post calibration              | calibration of models          | only recalibrates scores      | 2017 (NIPS)         |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | DUQ                    | Distance to class prototypes               | Distance from embedding      | Predict only if within        | Simple and fast;               | Limited to clear class        | van Amersfoort      |\n",
    "# |                        | gives predictive confidence                | to known class centroids     | decision boundary             | no sampling required           | boundaries; low flexibility   | et al., 2020        |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Focal Loss Ensembles   | Use focal loss to prioritize               | Internal confidence          | Aggregate only when           | Improves recall on rare        | Sensitive to tuning of        | Lin et al.,         |\n",
    "# |                        | hard/rare samples in ensemble              | based on loss weighting      | model is confident            | events; reduces overfitting    | focal loss params             | 2017 (RetinaNet)    |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "# | Trust Score Ensemble   | Post-hoc distance ratio                    | Ratio of distances to        | Select models where           | Interpretable; simple          | Needs external scoring        |                     |\n",
    "# |                        | for estimating prediction trust            | nearest labeled samples      | trust score exceeds threshold | and model-agnostic             | set; limited to clean regions |                     |\n",
    "# +------------------------+--------------------------------------------+------------------------------+-------------------------------+--------------------------------+-------------------------------+---------------------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540fb2f",
   "metadata": {},
   "source": [
    "<div style=\"font-size:10px\">\n",
    "\n",
    "| **Aspect** | **MLE (Maximum Likelihood Estimation) 最大似然估计** | **MAP (Maximum A Posteriori Estimation) 最大后验估计** |\n",
    "|------------|------------------------------------------|--------------------------------------------|\n",
    "| **Objective** | Estimate parameter $\\theta$ that maximizes the likelihood of the observed data. | Estimate parameter $\\theta$ that maximizes the posterior probability given the data. |\n",
    "| **Optimization Goal** | $\\displaystyle \\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} P(D \\mid \\theta)$ | $\\displaystyle \\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(\\theta \\mid D)$ |\n",
    "| **Formula Derivation** | Maximize the likelihood: <br> $\\displaystyle \\mathcal{L}(\\theta) = \\prod_{i=1}^{n} P(x_i \\mid \\theta)$ <br> Take the log: <br> $\\displaystyle \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^{n} \\log P(x_i \\mid \\theta)$ <br> Then: <br> $\\displaystyle \\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} \\log P(D \\mid \\theta)$ | Use Bayes’ Theorem: <br> $\\displaystyle P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}$ <br> Ignore constant $P(D)$: <br> $\\displaystyle \\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(D \\mid \\theta) P(\\theta)$ <br> or log-form: <br> $\\displaystyle \\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\left[ \\log P(D \\mid \\theta) + \\log P(\\theta) \\right]$ |\n",
    "| **Includes Prior?** | ❌ No | ✅ Yes |\n",
    "| **Sensitivity to Prior** | Not sensitive (no prior used) | Sensitive to prior choice |\n",
    "| **Overfitting Risk** | Higher, especially for small data | Lower, prior acts as regularizer |\n",
    "| **Asymptotic Behavior** | As $n \\to \\infty$, MLE is consistent | As $n \\to \\infty$, MAP $\\to$ MLE |\n",
    "| **Computational Complexity** | Lower (no prior term) | Higher (includes prior) |\n",
    "| **Interpretation** | Frequentist — parameters are fixed | Bayesian — parameters are random variables |\n",
    "| **Uniform Prior Case** | MLE = MAP | Yes, if $P(\\theta)$ is uniform |\n",
    "| **Regularization View** | No regularization | Prior acts like regularization <br> Gaussian prior $\\Rightarrow L_2$ <br> Laplace prior $\\Rightarrow L_1$ |\n",
    "| **Example: Gaussian Likelihood** | $x_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ <br> $\\displaystyle \\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n} \\sum x_i$ | Prior: $\\mu \\sim \\mathcal{N}(\\mu_0, \\tau^2)$ <br> $\\displaystyle \\hat{\\mu}_{\\text{MAP}} = \\frac{n\\sigma^{-2}}{n\\sigma^{-2} + \\tau^{-2}} \\bar{x} + \\frac{\\tau^{-2}}{n\\sigma^{-2} + \\tau^{-2}} \\mu_0$ |\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_curve, classification_report, confusion_matrix, accuracy_score\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label.parquet'))\n",
    "raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label_trend_1to8.parquet'))\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label_trend_3to24.parquet'))\n",
    "\n",
    "# df = raw_data[-(60*24*7):].copy()\n",
    "df = raw_data.copy()\n",
    "\n",
    "# H_PER_D = 23                                # trade hours per day\n",
    "# P_PER_B = 5                                 # equivalent bar period\n",
    "# EMA_VOL_SPAN = int(60/P_PER_B * H_PER_D)    # span for EMA volatility (daily)\n",
    "# CUSUM_FACTOR = 0.6                          # multiplier for CUSUM threshold\n",
    "# \n",
    "# # --- VOLATILITY AND FILTER ---\n",
    "# label = df[['label']].copy()\n",
    "# label['ref'] = np.log((df['close'] / df['close'].iloc[0]).fillna(1))\n",
    "# # EMA-based volatility on log returns\n",
    "# label['return'] = df['close'].pct_change().fillna(0)\n",
    "# # daily vol (volume/run bars has more constant volatility for their homoscedasticity)\n",
    "# label['pos_return'] = label['return'].where(label['return'] > 0, 0.0001)\n",
    "# label['neg_return'] = label['return'].where(label['return'] < 0, 0.0001)\n",
    "# label['pos_vol'] = label['pos_return'].ewm(span=EMA_VOL_SPAN, adjust=False).std().replace(0, np.nan).ffill().bfill()\n",
    "# label['neg_vol'] = label['neg_return'].ewm(span=EMA_VOL_SPAN, adjust=False).std().replace(0, np.nan).ffill().bfill()\n",
    "# \n",
    "# # CUSUM to mark breakout events and direction\n",
    "# s_pos, s_neg = 0.0, 0.0\n",
    "# label['event'] = 0.0 # np.nan\n",
    "# for i in range(1, len(label)):\n",
    "#     # note that for time i, the label/prediction is calculated after all info of that time is known\n",
    "#     pos_threshold = label['pos_vol'].iloc[i] * CUSUM_FACTOR\n",
    "#     neg_threshold = label['neg_vol'].iloc[i] * CUSUM_FACTOR\n",
    "#     diff = label['return'].iloc[i]\n",
    "#     s_pos = max(0, s_pos + diff)\n",
    "#     s_neg = min(0, s_neg + diff)\n",
    "#     index = label.index[i]\n",
    "#     if s_pos > pos_threshold:\n",
    "#         label.loc[index, 'event'] = 1\n",
    "#         s_pos = 0.0\n",
    "#     elif s_neg < -neg_threshold:\n",
    "#         label.loc[index, 'event'] = -1\n",
    "#         s_neg = 0.0\n",
    "# \n",
    "# index = label.index[label['event'] != 0]\n",
    "# \n",
    "# print(df.shape)\n",
    "# df.drop(columns=['open', 'high', 'low', 'close', 'uniqueness'], axis=1, inplace=True)\n",
    "# df = df.loc[index]\n",
    "# print(df.shape)\n",
    "\n",
    "# | σ    | Cumulative Probability | Approx. % within ±σ range          |\n",
    "# |------|------------------------|------------------------------------|\n",
    "# | 0.1  | 0.0797                 | ~7.97% within ±0.1σ                |\n",
    "# | 0.2  | 0.1587                 | ~15.87% within ±0.2σ               |\n",
    "# | 0.3  | 0.2266                 | ~22.66% within ±0.3σ               |\n",
    "# | 0.4  | 0.3108                 | ~31.08% within ±0.4σ               |\n",
    "# | 0.5  | 0.3829                 | ~38.29% within ±0.5σ               |\n",
    "# | 1.0  | 0.6827                 | ~68.27% within ±1σ (1-sigma rule)  |\n",
    "# | 1.5  | 0.8664                 | ~86.64% within ±1.5σ               |\n",
    "# | 2.0  | 0.9545                 | ~95.45% within ±2σ (2-sigma rule)  |\n",
    "# | 2.5  | 0.9876                 | ~98.76% within ±2.5σ               |\n",
    "\n",
    "label = df['label']\n",
    "sigma = label.std()*0.5 # assume normal distribution (actually scaled version)\n",
    "def classify(x):\n",
    "    if x < -sigma:\n",
    "        return -1\n",
    "    elif x > sigma:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "y = df['label'].apply(classify)\n",
    "X = df.drop(columns=['label'], axis=1)\n",
    "\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split data into training, validation and test sets\n",
    "X_training_test = X\n",
    "y_training_test = y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training_test, y_training_test, test_size=0.2, shuffle=False)\n",
    "\n",
    "n_estimator = 100\n",
    "depth = 5\n",
    "c_random_state = 42\n",
    "\n",
    "# Random Forest Model\n",
    "rf = RandomForestClassifier(max_depth=depth, n_estimators=n_estimator, oob_score=True,\n",
    "                            criterion='entropy', random_state=c_random_state)\n",
    "rf.fit(X_train, y_train.values.ravel())\n",
    "print(\"Out-of-bag Accuracy (OOB Score): {:.6f}\".format(rf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f953874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold\n",
    "no_of_folds = 5\n",
    "kfold = KFold(shuffle=True, random_state=1, n_splits=no_of_folds)\n",
    "print(kfold)\n",
    "\n",
    "accuracy_array = np.zeros(no_of_folds)\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X_training_test.values):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    rf.fit(X_train, np.array(y_train).ravel())\n",
    "\n",
    "    y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy_array[i] = accuracy_score(y_test, y_pred)\n",
    "    i += 1\n",
    "    # print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(accuracy_array)\n",
    "print(\"Mean KFold accuracy: {:.6f}\".format(np.mean(accuracy_array)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
