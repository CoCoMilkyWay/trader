{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c21c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first use Tsfresh/Tsfel to generate some features to get a feel of what we are dealing with:\n",
    "#   1. Tsfresh takes n*(id, time, features) and len(id) labels as input:\n",
    "#   2. tsfresh doesn't test/preprocess anything about input timeseries, so you better make them stationary by yourselves\n",
    "#   3. id = independent timeseries, long or short\n",
    "#   4. each id has 1 label, which is against the notion that timeseries need to have timeseries label as well\n",
    "#   5. however, at each time stamp, we have short memory features and long memory features, for short memory features, it is actually equivalent to having a sliding window of short timeseries and output a series of scalar labels, which is exactly how tsfresh/tsfel works\n",
    "#   6. indeed, it might be the best practice to use these tools to generate short memory features and handcraft long memory/more hidden features\n",
    "#   7. these tools can not catch any cross-sectional features between ids. you can generate features for 1 id at a time, works exactly the same, it is just more computationally efficient for parallelism\n",
    "#   8. do features on long timeseries works equally well on its splitted many rolling short timeseries on average? not necessary\n",
    "#   9. thus how do you evaluate whether features generated like this work consistently over time? either averaging statistical performance or try to train a model(then compare model weights)\n",
    "#   10.try short->long window for feature importance(e.g. FFT doesn't work well on short window length)\n",
    "#   11.to evaluate the effect of a feature, dont need to have too many, have enough samples that can make sure feature is stationary and preferably normal distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca03dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | **Method**                                       | **Approach Type**           | **Pattern Type Learned**                 | **Interpretable?** | **Best Use Case**                                  | **Tools / Libraries**                |\n",
    "# | ------------------------------------------------ | --------------------------- | ---------------------------------------- | ------------------ | -------------------------------------------------- | ------------------------------------ |\n",
    "# | **Shapelet Transform**                           | Distance-based (Supervised) | Local subsequence \"shapes\"               | ✅ High            | Finding interpretable, discriminative patterns     | `tslearn`, `sktime`, `pyts`          |\n",
    "# | **Dynamic Time Warping + Supervised Clustering** | Similarity + Aggregation    | Whole series similarity (flexible time)  | ✅ Medium          | Pattern grouping + average label scoring           | `tslearn`, `dtaidistance`, `HDBSCAN` |\n",
    "# | **Bag-of-SFA Symbols (BOSS)**                    | Symbolic / Frequency        | Frequency of symbolic subsequences       | ✅ High            | Symbolic pattern detection (e.g., zigzags)         | `sktime`, `pyts`                     |\n",
    "# | **TDE (Temporal Dictionary Ensemble)**           | Dictionary Learning         | Frequency/strength of learned shapes     | ✅ High            | Detecting repeated motifs with outcome correlation | `sktime`                             |\n",
    "# | **Time Series Forest (TSF)**                     | Tree Ensemble               | Random intervals + summaries             | ⚠️ Partial         | Strong classification baseline                     | `sktime`, `tslearn`                  |\n",
    "# | **HIVE-COTE 2.0**                                | Ensemble (Hybrid)           | Multiple feature types                   | ✅ Partial         | State-of-the-art accuracy on many tasks            | `sktime`                             |\n",
    "# | **ROCKET / MiniROCKET / MultiROCKET**            | Random Kernels              | Statistical response to many filters     | ❌ No              | Fast, accurate classification/regression           | `sktime`, `rocket-boost`             |\n",
    "# | **1D CNNs**                                      | Deep Learning (CNN)         | Localized filters (motifs)               | ⚠️ Limited         | Predicting from raw time series                    | `Keras`, `PyTorch`                   |\n",
    "# | **RNNs / LSTMs / Transformers**                  | Deep Learning (Sequential)  | Temporal dynamics, memory, long patterns | ❌ No              | Capturing long-term dependencies                   | `PyTorch`, `Keras`, `Hugging Face`   |\n",
    "# | **Siamese / Triplet Networks**                   | Metric Learning             | Latent similarity between time windows   | ⚠️ Medium          | Learning similarity among high-performing windows  | `PyTorch`, `TensorFlow`              |\n",
    "# | **Autoencoder + Regressor**                      | Latent Representation       | Abstract embeddings                      | ⚠️ Medium          | Unsupervised pretraining + label prediction        | `PyTorch`, `scikit-learn`            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe72910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.956024e+04</td>\n",
       "      <td>1026.461275</td>\n",
       "      <td>16500.500000</td>\n",
       "      <td>18946.750000</td>\n",
       "      <td>19636.500000</td>\n",
       "      <td>20201.687500</td>\n",
       "      <td>21521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.957602e+04</td>\n",
       "      <td>1021.795364</td>\n",
       "      <td>16534.500000</td>\n",
       "      <td>18965.062500</td>\n",
       "      <td>19648.500000</td>\n",
       "      <td>20215.125000</td>\n",
       "      <td>21529.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.954397e+04</td>\n",
       "      <td>1030.919129</td>\n",
       "      <td>16452.500000</td>\n",
       "      <td>18928.000000</td>\n",
       "      <td>19624.000000</td>\n",
       "      <td>20185.062500</td>\n",
       "      <td>21513.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.956023e+04</td>\n",
       "      <td>1026.465353</td>\n",
       "      <td>16499.500000</td>\n",
       "      <td>18946.437500</td>\n",
       "      <td>19636.250000</td>\n",
       "      <td>20202.312500</td>\n",
       "      <td>21521.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>4.428075e-01</td>\n",
       "      <td>0.496731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_ret</th>\n",
       "      <td>20159.0</td>\n",
       "      <td>-2.444558e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom1</th>\n",
       "      <td>20159.0</td>\n",
       "      <td>5.933983e-07</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>-0.045618</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.025041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom2</th>\n",
       "      <td>20158.0</td>\n",
       "      <td>1.188938e-06</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>-0.048490</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.030751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom3</th>\n",
       "      <td>20157.0</td>\n",
       "      <td>1.803692e-06</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>-0.052876</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.040459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom4</th>\n",
       "      <td>20156.0</td>\n",
       "      <td>2.418886e-06</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>-0.053822</td>\n",
       "      <td>-0.001254</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mom5</th>\n",
       "      <td>20155.0</td>\n",
       "      <td>3.026053e-06</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>-0.052905</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.048939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>1.080068e-03</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.007921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_1</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.625446e-02</td>\n",
       "      <td>0.139996</td>\n",
       "      <td>-0.634113</td>\n",
       "      <td>-0.118942</td>\n",
       "      <td>-0.025649</td>\n",
       "      <td>0.069316</td>\n",
       "      <td>0.475915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_2</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-1.499542e-02</td>\n",
       "      <td>0.138863</td>\n",
       "      <td>-0.447951</td>\n",
       "      <td>-0.112155</td>\n",
       "      <td>-0.019590</td>\n",
       "      <td>0.082092</td>\n",
       "      <td>0.545940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_3</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.895765e-02</td>\n",
       "      <td>0.139141</td>\n",
       "      <td>-0.497556</td>\n",
       "      <td>-0.123967</td>\n",
       "      <td>-0.029113</td>\n",
       "      <td>0.066792</td>\n",
       "      <td>0.467896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_4</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.548542e-02</td>\n",
       "      <td>0.147383</td>\n",
       "      <td>-0.495312</td>\n",
       "      <td>-0.129528</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>0.080143</td>\n",
       "      <td>0.506904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autocorr_5</th>\n",
       "      <td>20110.0</td>\n",
       "      <td>-2.338549e-02</td>\n",
       "      <td>0.147182</td>\n",
       "      <td>-0.558187</td>\n",
       "      <td>-0.122946</td>\n",
       "      <td>-0.023834</td>\n",
       "      <td>0.077938</td>\n",
       "      <td>0.472844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t1</th>\n",
       "      <td>20158.0</td>\n",
       "      <td>-2.560565e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t2</th>\n",
       "      <td>20157.0</td>\n",
       "      <td>-2.543307e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t3</th>\n",
       "      <td>20156.0</td>\n",
       "      <td>-2.670947e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t4</th>\n",
       "      <td>20155.0</td>\n",
       "      <td>-3.169885e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_t5</th>\n",
       "      <td>20154.0</td>\n",
       "      <td>-3.210667e-07</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.046691</td>\n",
       "      <td>-0.000659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.024732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fast_mavg</th>\n",
       "      <td>20154.0</td>\n",
       "      <td>1.955967e+04</td>\n",
       "      <td>1025.773046</td>\n",
       "      <td>16533.821429</td>\n",
       "      <td>18943.187500</td>\n",
       "      <td>19637.803571</td>\n",
       "      <td>20198.312500</td>\n",
       "      <td>21508.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slow_mavg</th>\n",
       "      <td>20146.0</td>\n",
       "      <td>1.955893e+04</td>\n",
       "      <td>1024.922277</td>\n",
       "      <td>16565.583333</td>\n",
       "      <td>18942.870833</td>\n",
       "      <td>19639.958333</td>\n",
       "      <td>20197.787500</td>\n",
       "      <td>21502.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atr</th>\n",
       "      <td>20160.0</td>\n",
       "      <td>3.206390e+01</td>\n",
       "      <td>16.285737</td>\n",
       "      <td>11.114555</td>\n",
       "      <td>22.750606</td>\n",
       "      <td>27.546002</td>\n",
       "      <td>35.127528</td>\n",
       "      <td>204.381303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean          std           min           25%  \\\n",
       "open        20160.0  1.956024e+04  1026.461275  16500.500000  18946.750000   \n",
       "high        20160.0  1.957602e+04  1021.795364  16534.500000  18965.062500   \n",
       "low         20160.0  1.954397e+04  1030.919129  16452.500000  18928.000000   \n",
       "close       20160.0  1.956023e+04  1026.465353  16499.500000  18946.437500   \n",
       "label       20160.0  4.428075e-01     0.496731      0.000000      0.000000   \n",
       "log_ret     20159.0 -2.444558e-07     0.001295     -0.046691     -0.000659   \n",
       "mom1        20159.0  5.933983e-07     0.001294     -0.045618     -0.000659   \n",
       "mom2        20158.0  1.188938e-06     0.001830     -0.048490     -0.000909   \n",
       "mom3        20157.0  1.803692e-06     0.002260     -0.052876     -0.001099   \n",
       "mom4        20156.0  2.418886e-06     0.002603     -0.053822     -0.001254   \n",
       "mom5        20155.0  3.026053e-06     0.002912     -0.052905     -0.001446   \n",
       "volatility  20110.0  1.080068e-03     0.000716      0.000309      0.000710   \n",
       "autocorr_1  20110.0 -2.625446e-02     0.139996     -0.634113     -0.118942   \n",
       "autocorr_2  20110.0 -1.499542e-02     0.138863     -0.447951     -0.112155   \n",
       "autocorr_3  20110.0 -2.895765e-02     0.139141     -0.497556     -0.123967   \n",
       "autocorr_4  20110.0 -2.548542e-02     0.147383     -0.495312     -0.129528   \n",
       "autocorr_5  20110.0 -2.338549e-02     0.147182     -0.558187     -0.122946   \n",
       "log_t1      20158.0 -2.560565e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t2      20157.0 -2.543307e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t3      20156.0 -2.670947e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t4      20155.0 -3.169885e-07     0.001295     -0.046691     -0.000659   \n",
       "log_t5      20154.0 -3.210667e-07     0.001295     -0.046691     -0.000659   \n",
       "fast_mavg   20154.0  1.955967e+04  1025.773046  16533.821429  18943.187500   \n",
       "slow_mavg   20146.0  1.955893e+04  1024.922277  16565.583333  18942.870833   \n",
       "atr         20160.0  3.206390e+01    16.285737     11.114555     22.750606   \n",
       "\n",
       "                     50%           75%           max  \n",
       "open        19636.500000  20201.687500  21521.000000  \n",
       "high        19648.500000  20215.125000  21529.750000  \n",
       "low         19624.000000  20185.062500  21513.000000  \n",
       "close       19636.250000  20202.312500  21521.750000  \n",
       "label           0.000000      1.000000      1.000000  \n",
       "log_ret         0.000000      0.000649      0.024732  \n",
       "mom1            0.000000      0.000649      0.025041  \n",
       "mom2            0.000000      0.000889      0.030751  \n",
       "mom3            0.000000      0.001070      0.040459  \n",
       "mom4           -0.000013      0.001221      0.047852  \n",
       "mom5           -0.000025      0.001383      0.048939  \n",
       "volatility      0.000878      0.001141      0.007921  \n",
       "autocorr_1     -0.025649      0.069316      0.475915  \n",
       "autocorr_2     -0.019590      0.082092      0.545940  \n",
       "autocorr_3     -0.029113      0.066792      0.467896  \n",
       "autocorr_4     -0.028310      0.080143      0.506904  \n",
       "autocorr_5     -0.023834      0.077938      0.472844  \n",
       "log_t1          0.000000      0.000649      0.024732  \n",
       "log_t2          0.000000      0.000649      0.024732  \n",
       "log_t3          0.000000      0.000649      0.024732  \n",
       "log_t4          0.000000      0.000649      0.024732  \n",
       "log_t5          0.000000      0.000649      0.024732  \n",
       "fast_mavg   19637.803571  20198.312500  21508.035714  \n",
       "slow_mavg   19639.958333  20197.787500  21502.150000  \n",
       "atr            27.546002     35.127528    204.381303  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple features\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label.parquet'))\n",
    "raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label(trend_1to8).parquet'))\n",
    "# raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label(trend_3to24).parquet'))\n",
    "raw_data = raw_data[-(60*24*14):].copy()\n",
    "\n",
    "# Log Returns\n",
    "raw_data['log_ret'] = np.log(raw_data['close'] / raw_data['close'].shift(1))\n",
    "\n",
    "# Momentum\n",
    "for i in range(1, 6):\n",
    "    raw_data[f'mom{i}'] = raw_data['close'].pct_change(periods=i)\n",
    "\n",
    "# Volatility\n",
    "window_stdev = 50\n",
    "raw_data['volatility'] = raw_data['log_ret'].rolling(window=window_stdev, min_periods=window_stdev).std()\n",
    "\n",
    "# Serial Correlation\n",
    "window_autocorr = 50\n",
    "for lag in range(1, 6):\n",
    "    raw_data[f'autocorr_{lag}'] = raw_data['log_ret'].rolling(window=window_autocorr, min_periods=window_autocorr).apply(\n",
    "        lambda x: x.autocorr(lag=lag) if x.notna().sum() > lag else np.nan,\n",
    "        raw=False\n",
    "    )\n",
    "\n",
    "# Lagged log returns\n",
    "for i in range(1, 6):\n",
    "    raw_data[f'log_t{i}'] = raw_data['log_ret'].shift(i)\n",
    "\n",
    "# Moving averages\n",
    "fast_window = 7\n",
    "slow_window = 15\n",
    "raw_data['fast_mavg'] = raw_data['close'].rolling(window=fast_window, min_periods=fast_window).mean()\n",
    "raw_data['slow_mavg'] = raw_data['close'].rolling(window=slow_window, min_periods=slow_window).mean()\n",
    "\n",
    "# ATR\n",
    "high_low = raw_data['high'] - raw_data['low']\n",
    "high_close = (raw_data['high'] - raw_data['close'].shift()).abs()\n",
    "low_close = (raw_data['low'] - raw_data['close'].shift()).abs()\n",
    "tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "raw_data['atr'] = tr.ewm(span=12, adjust=False).mean()\n",
    "\n",
    "# raw_data.info(verbose=True, memory_usage='deep')\n",
    "raw_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf622116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id          time     value\n",
      "10074  10064  202505160937  0.000453\n",
      "10075  10064  202505161008  0.000334\n",
      "10076  10064  202505161028  0.001015\n",
      "10077  10064  202505161056  0.000939\n",
      "10078  10064  202505161139  0.000607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [06:21<00:00, 12.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid features number:320\n",
      "\n",
      "Top statistically important features (Spearman correlation):\n",
      "value__number_crossing_m__m_0: 0.0464\n",
      "value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.0: 0.0359\n",
      "value__permutation_entropy__dimension_7__tau_1: 0.0352\n",
      "value__permutation_entropy__dimension_6__tau_1: 0.0316\n",
      "value__symmetry_looking__r_0.15000000000000002: 0.0316\n",
      "value__cwt_coefficients__coeff_14__w_2__widths_(2, 5, 10, 20): 0.0307\n",
      "value__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.2: 0.0303\n",
      "value__change_quantiles__f_agg_\"var\"__isabs_True__qh_1.0__ql_0.0: 0.0293\n",
      "value__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.2__ql_0.0: 0.0286\n",
      "value__number_peaks__n_1: 0.0284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTop statistically important features (Spearman correlation):\\nclose__has_duplicate_max: 0.0721\\nclose__spkt_welch_density__coeff_5: 0.0693\\nclose__symmetry_looking__r_0.30000000000000004: 0.0532\\nclose__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.4: 0.0521\\nclose__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.6: 0.0505 \\nclose__large_standard_deviation__r_0.4: 0.0477\\nclose__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"var\": 0.0474  \\nclose__partial_autocorrelation__lag_4: 0.0434\\nclose__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.2__ql_0.0: 0.0420\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tsfresh Features\n",
    "# https://tsfresh.readthedocs.io/en/latest/text/list_of_features.html\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# print in full\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# period < 20 = stationary (ADF test)\n",
    "# period > 10 = significant trend info (correlation)\n",
    "\n",
    "M = 200  # number of top_features kept\n",
    "weeks = 5\n",
    "period = 15\n",
    "step = 1  # should be 1\n",
    "\n",
    "dir = os.path.join(os.getcwd())\n",
    "# dir = os.path.dirname(os.path.abspath(__file__))\n",
    "filepath = os.path.join(dir, f\"bar_and_label(trend_3to24).parquet\")\n",
    "\n",
    "# 'bar_and_label(calmar).parquet'\n",
    "# 'bar_and_label(trend_1to8).parquet'\n",
    "# 'bar_and_label(trend_3to24).parquet'\n",
    "\n",
    "\n",
    "def analyze_feature(df: pd.DataFrame):\n",
    "    series = df[\"close\"]\n",
    "    df['ref'] = np.log((series / series.iloc[0]).fillna(1))\n",
    "    df['value'] = np.log((series / series.rolling(period).mean().shift(period)).fillna(1))\n",
    "\n",
    "    # print(df.tail())\n",
    "    df = df[-int(60/5*24*7*weeks):].copy()\n",
    "    df = df.sort_values('time').reset_index()\n",
    "\n",
    "    # Rolling the time series\n",
    "    window_size = period\n",
    "    step_size = step\n",
    "    segments = []\n",
    "    labels = []\n",
    "    times = []\n",
    "    for idx, start in enumerate(range(0, len(df) - window_size, step_size)):\n",
    "        end = start + window_size\n",
    "        segment = df.iloc[start:end].copy()\n",
    "        segment[\"id\"] = idx  # unique id for each window\n",
    "        segments.append(segment[[\"id\", \"time\", \"value\"]])\n",
    "        labels.append(df[\"label\"].iloc[end])\n",
    "        times.append(df[\"time\"].iloc[end])\n",
    "\n",
    "    X_features = pd.concat(segments, axis=0)\n",
    "    y_labels = pd.Series(labels)\n",
    "    t_times = pd.Series(times)\n",
    "\n",
    "    print(X_features.tail())\n",
    "    # print(y_labels.tail())\n",
    "\n",
    "    # --- Feature extraction\n",
    "    X_features = extract_features(\n",
    "        X_features,\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"time\",\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        show_warnings=False,\n",
    "        impute_function=impute,  # NOTE: tsfresh's impute implementation may leak future info, but mostly okay\n",
    "        # disable_progressbar=True\n",
    "    )\n",
    "    X_features = pd.DataFrame(X_features)  # Ensure features is a DataFrame\n",
    "    X_features = X_features.loc[:, X_features.notna().sum() == len(X_features)]  # remove NaN features after impute\n",
    "    X_features = X_features.loc[:, X_features.nunique() > 1]  # remove constant features\n",
    "    print(f\"Valid features number:{X_features.shape[1]}\")\n",
    "    # X_features = select_features(X_features, y_labels)\n",
    "    # print(X_features.describe().transpose())\n",
    "\n",
    "    # | Feature Type | Target Type | Statistical Test Used             |\n",
    "    # | ------------ | ----------- | --------------------------------- |\n",
    "    # | Continuous   | Binary      | Kolmogorov-Smirnov test (KS test) |\n",
    "    # | Continuous   | Categorical | ANOVA F-test                      |\n",
    "    # | Continuous   | Continuous  | Kendall's tau correlation test    |\n",
    "    # | Binary       | Binary      | Fisher’s exact test               |\n",
    "    # | Binary       | Categorical | Chi-square test                   |\n",
    "    # | Binary       | Continuous  | Point biserial correlation        |\n",
    "\n",
    "    # --- Compute correlation with label\n",
    "    results = {}\n",
    "    for col in X_features.columns:\n",
    "        # Use Spearman for robustness (nonlinear monotonic relationships)\n",
    "        corr = pd.Series(X_features[col]).corr(y_labels, method='spearman')\n",
    "        results[col] = abs(corr)  # use absolute value to reflect strength\n",
    "\n",
    "    # --- Sort by importance\n",
    "    sorted_features = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop statistically important features (Spearman correlation):\")\n",
    "    for name, score in sorted_features[:10]:\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "    top_features = [name for name, score in sorted_features[:M]]\n",
    "    X_selected = X_features[top_features].copy()\n",
    "    X_selected[\"time\"] = t_times.values\n",
    "    X_selected[\"label\"] = y_labels.values\n",
    "    X_selected.set_index('time', inplace=True)\n",
    "    # X_selected.to_parquet(os.path.join(dir, f\"features_and_label(tsfresh_trend_3to24).parquet\"))\n",
    "    # print(X_selected.describe().transpose())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_parquet(filepath)\n",
    "    analyze_feature(df)\n",
    "\n",
    "\"\"\"\n",
    "Top statistically important features (Spearman correlation):\n",
    "close__has_duplicate_max: 0.0721\n",
    "close__spkt_welch_density__coeff_5: 0.0693\n",
    "close__symmetry_looking__r_0.30000000000000004: 0.0532\n",
    "close__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.6__ql_0.4: 0.0521\n",
    "close__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.6: 0.0505 \n",
    "close__large_standard_deviation__r_0.4: 0.0477\n",
    "close__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"var\": 0.0474  \n",
    "close__partial_autocorrelation__lag_4: 0.0434\n",
    "close__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.2__ql_0.0: 0.0420\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26ce1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple candlestick pattern\n",
    "import array\n",
    "import math\n",
    "\n",
    "\n",
    "class candlestrength:\n",
    "    \"\"\"\n",
    "    Analyze candlestick strength based on body position within true range.\n",
    "\n",
    "    Strength patterns explained (│ = wick, █ = body):\n",
    "\n",
    "    Bullish Patterns (close > open):\n",
    "    Strength 0 (Very Bullish):     Strength 1:           Strength 2:           Strength 3:           Strength 4:\n",
    "\n",
    "        █                             │                      │                      │                      │   \n",
    "        █       Bottom third          █     Bottom third     │     Middle third     █    Middle third      -    Top third\n",
    "        █                             █                      █                      │                      │   \n",
    "\n",
    "    Bearish Patterns (close < open):\n",
    "    Strength 8 (Very Bearish):     Strength 7:           Strength 6:           Strength 5:           Strength 4:\n",
    "        █                             █                      █                      │                      │   \n",
    "        █         Top third           █      Top third       │    Middle third      █    Middle third      -    Bottom third\n",
    "        █                             │                      │                      │                      │   \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 opens: array.array,\n",
    "                 highs: array.array,\n",
    "                 lows: array.array,\n",
    "                 closes: array.array,\n",
    "                 ):\n",
    "        \"\"\"Initialize the CandleStrength analyzer.\"\"\"\n",
    "        self.opens = opens\n",
    "        self.highs = highs\n",
    "        self.lows = lows\n",
    "        self.closes = closes\n",
    "\n",
    "        self.strength = array.array('b', [])\n",
    "        self.is_bullish = None\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update and calculate the candlestick strength.\n",
    "\n",
    "        Args:\n",
    "            open_price (float): Opening price\n",
    "            high (float): High price\n",
    "            low (float): Low price\n",
    "            close (float): Closing price\n",
    "\n",
    "        Returns:\n",
    "            int: Strength rating from 0 (most bullish) to 8 (most bearish)\n",
    "        \"\"\"\n",
    "        open = self.opens[-1]\n",
    "        high = self.highs[-1]\n",
    "        low = self.lows[-1]\n",
    "        close = self.closes[-1]\n",
    "        # Calculate true range and section sizes\n",
    "        true_range = high - low\n",
    "        section_size = true_range / 3\n",
    "\n",
    "        # Calculate section boundaries\n",
    "        lower_third = low + section_size\n",
    "        upper_third = high - section_size\n",
    "\n",
    "        # Determine body position\n",
    "        body_high = max(open, close)\n",
    "        body_low = min(open, close)\n",
    "\n",
    "        # Determine if candle is bullish or bearish\n",
    "        self.is_bullish = close > open\n",
    "\n",
    "        # Calculate strength based on body position\n",
    "        if self.is_bullish:\n",
    "            if body_high <= lower_third:\n",
    "                strength = 0  # Very bullish - full body in bottom third\n",
    "            elif body_low <= lower_third:\n",
    "                strength = 1  # Body extends into bottom third\n",
    "            elif body_high <= upper_third:\n",
    "                strength = 2  # Full body in middle third\n",
    "            elif body_low <= upper_third:\n",
    "                strength = 3  # Body extends into middle third\n",
    "            else:\n",
    "                strength = 4  # Body in top third\n",
    "        else:  # bearish\n",
    "            if body_low >= upper_third:\n",
    "                strength = 8  # Very bearish - full body in top third\n",
    "            elif body_high >= upper_third:\n",
    "                strength = 7  # Body extends into top third\n",
    "            elif body_low >= lower_third:\n",
    "                strength = 6  # Full body in middle third\n",
    "            elif body_high >= lower_third:\n",
    "                strength = 5  # Body extends into middle third\n",
    "            else:\n",
    "                strength = 4  # Body in bottom third\n",
    "\n",
    "        self.strength.append(strength)\n",
    "        LEN = 100\n",
    "        if len(self.strength) > 2*LEN:\n",
    "            del self.strength[:-LEN]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pytorch Tensor: (timestamp(413636), feature(10) + label(1), codes(1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413636/413636 [01:12<00:00, 5677.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              count      mean       std       min       25%       50%       75%       max\n",
      "logreturn_log_returns_1    413636.0  0.000003  0.000947 -0.060974 -0.000496  0.000016  0.000504  0.025635\n",
      "logreturn_log_returns_2    413636.0  0.000003  0.000947 -0.060974 -0.000496  0.000016  0.000504  0.025635\n",
      "logreturn_log_returns_3    413636.0  0.000003  0.000947 -0.060974 -0.000496  0.000016  0.000504  0.025635\n",
      "logreturn_log_returns_4    413636.0  0.000003  0.000947 -0.060974 -0.000496  0.000016  0.000504  0.025635\n",
      "logreturn_log_returns_5    413636.0  0.000003  0.000947 -0.060974 -0.000496  0.000016  0.000504  0.025635\n",
      "candlestrength_strength_1  413636.0  4.002007  2.623760  0.000000  1.000000  4.000000  7.000000  8.000000\n",
      "candlestrength_strength_2  413636.0  4.002024  2.623764  0.000000  1.000000  4.000000  7.000000  8.000000\n",
      "candlestrength_strength_3  413636.0  4.002016  2.623768  0.000000  1.000000  4.000000  7.000000  8.000000\n",
      "candlestrength_strength_4  413636.0  4.002033  2.623771  0.000000  1.000000  4.000000  7.000000  8.000000\n",
      "candlestrength_strength_5  413636.0  4.002043  2.623775  0.000000  1.000000  4.000000  7.000000  8.000000\n",
      "label                      413636.0  0.447055  0.497190  0.000000  0.000000  0.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# bar features (model baseline tests)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import array\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(dir, \"../..\")))\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "class TimeSeries_Analysis():\n",
    "    def __init__(self, n_timestamps: int):\n",
    "        from Math.performance.log_return import logreturn\n",
    "\n",
    "        self.n_timestamps = n_timestamps\n",
    "\n",
    "        # Initialize fixed-size arrays for each level\n",
    "        self.multipliers = [1]\n",
    "        n_timeframes = len(self.multipliers)\n",
    "        self.counts = array.array('I', [0] * n_timeframes)\n",
    "        self.opens = [array.array('d', [0.0]) for _ in range(n_timeframes)]  # trimmed as size increases\n",
    "        self.highs = [array.array('d', [0.0]) for _ in range(n_timeframes)]  # trimmed as size increases\n",
    "        self.lows = [array.array('d', [0.0]) for _ in range(n_timeframes)]   # trimmed as size increases\n",
    "        self.closes = [array.array('d', [0.0]) for _ in range(n_timeframes)]  # trimmed as size increases\n",
    "        self.volumes = [array.array('L', [0]) for _ in range(n_timeframes)]  # trimmed as size increases\n",
    "        self.timestamp = array.array('d', [0.0])  # put in array to be mutable\n",
    "\n",
    "        self.logreturn = logreturn(self.closes[i])\n",
    "        self.candlestrength = candlestrength(self.opens[i], self.highs[i], self.lows[i], self.closes[i])\n",
    "\n",
    "        self.feature_specs = {\n",
    "            'logreturn': {\n",
    "                'instance': self.logreturn,\n",
    "                'features': [('log_returns', -1), ('log_returns', -2), ('log_returns', -3), ('log_returns', -4), ('log_returns', -5)],\n",
    "                # 'Scaler': ScalingMethod.ROBUST,\n",
    "            },\n",
    "            'candlestrength': {\n",
    "                'instance': self.candlestrength,\n",
    "                'features': [('strength', -1), ('strength', -2), ('strength', -3), ('strength', -4), ('strength', -5),],\n",
    "                # 'Scaler': ScalingMethod.ROBUST,\n",
    "            },\n",
    "        }\n",
    "        self.n_features = sum(len(spec['features']) for spec in self.feature_specs.values())\n",
    "\n",
    "        self.init_shared_tensor()\n",
    "\n",
    "        self.init = False\n",
    "\n",
    "    def init_shared_tensor(self):\n",
    "        N_timestamps = self.n_timestamps\n",
    "        N_features = self.n_features\n",
    "        N_labels = 1\n",
    "        N_columns = N_features + N_labels\n",
    "        N_codes = 1\n",
    "        self.label_index = N_features\n",
    "        self.column_names = self._get_column_names()\n",
    "        print(f\"Initializing Pytorch Tensor: (timestamp({N_timestamps}), feature({N_features}) + label({N_labels}), codes({N_codes}))\")\n",
    "        self.shared_tensor = torch.zeros((N_timestamps, N_columns, N_codes), dtype=torch.float16).share_memory_()\n",
    "        self.time_tensor = torch.zeros(N_timestamps, dtype=torch.int64).share_memory_()\n",
    "\n",
    "    def analyze(self, code_idx, timestamp, open, high, low, close, label):\n",
    "        self.parse_kline(timestamp, open, high, low, close)\n",
    "        self.update_features(code_idx)\n",
    "        self.shared_tensor[self.counts[i], self.label_index, code_idx] = label\n",
    "        self.time_tensor[self.counts[i]] = timestamp\n",
    "        self.counts[i] += 1\n",
    "\n",
    "    def parse_kline(self, timestamp, open, high, low, close):\n",
    "        LEN = 100\n",
    "\n",
    "        # Append new bar\n",
    "        self.opens[i].append(open)\n",
    "        self.highs[i].append(high)\n",
    "        self.lows[i].append(low)\n",
    "        self.closes[i].append(close)\n",
    "        # self.volumes[i].append(curr_vol)\n",
    "        self.timestamp.append(timestamp)  # else idx\n",
    "\n",
    "        # Trim arrays to fixed window\n",
    "        if len(self.opens[i]) > 2 * LEN:\n",
    "            del self.opens[i][:-LEN]\n",
    "            del self.highs[i][:-LEN]\n",
    "            del self.lows[i][:-LEN]\n",
    "            del self.closes[i][:-LEN]\n",
    "            # del self.volumes[i][:-LEN]\n",
    "            del self.timestamp[:-LEN]\n",
    "\n",
    "    def update_features(self, code_idx):\n",
    "        for spec in self.feature_specs.values():\n",
    "            try:\n",
    "                spec['instance'].update()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if not self.init:\n",
    "            if len(self.closes[i]) <= 5:\n",
    "                return\n",
    "            else:\n",
    "                self.init = True\n",
    "\n",
    "        feature_idx = 0\n",
    "        for spec in self.feature_specs.values():\n",
    "            instance = spec['instance']\n",
    "            for attr_name, idx in spec['features']:\n",
    "                value = getattr(instance, attr_name)\n",
    "                self.shared_tensor[self.counts[i], feature_idx, code_idx] = value[idx] if idx is not None else value\n",
    "                feature_idx += 1\n",
    "\n",
    "    def _get_column_names(self):\n",
    "        names = []\n",
    "        for spec_key, spec in self.feature_specs.items():\n",
    "            for attr_name, idx in spec['features']:\n",
    "                name = f\"{spec_key}_{attr_name}_{abs(idx) if idx else 0}\"\n",
    "                names.append(name)\n",
    "        names.append(\"label\")\n",
    "        return names\n",
    "\n",
    "    def get_df(self, code_index: int):\n",
    "        if code_index:\n",
    "            data = self.shared_tensor[:, :, code_index].cpu().numpy()\n",
    "        else:\n",
    "            data = self.shared_tensor.squeeze(-1).cpu().numpy()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=self.column_names)\n",
    "        df['time'] = self.time_tensor.cpu().numpy()\n",
    "        df.set_index('time', inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label(calmar).parquet'))\n",
    "    # raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label(trend_1to8).parquet'))\n",
    "    raw_data = pd.read_parquet(os.path.join(os.getcwd(), 'bar_and_label(trend_3to24).parquet'))\n",
    "    # raw_data = raw_data[-(n_timestamps):].copy()\n",
    "\n",
    "    # n_timestamps = raw_data.shape[0]\n",
    "    n_timestamps = 12*24*20*1\n",
    "\n",
    "    TA = TimeSeries_Analysis(n_timestamps)\n",
    "\n",
    "    raw_data.reset_index(inplace=True)\n",
    "    for row in tqdm(raw_data[-(n_timestamps):].itertuples(index=False), total=n_timestamps):\n",
    "        TA.analyze(\n",
    "            code_idx=0,\n",
    "            timestamp=row.time,\n",
    "            open=row.open,\n",
    "            high=row.high,\n",
    "            low=row.low,\n",
    "            close=row.close,\n",
    "            label=row.label,\n",
    "        )\n",
    "\n",
    "    features_and_labels = TA.get_df(code_index=0)\n",
    "    print(features_and_labels.astype('float32').describe().transpose())  # describe function doesnt work for large number of f16\n",
    "    # features_and_labels.to_parquet(os.path.join(dir, f\"features_and_label(candlestick_trend_3to24).parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
