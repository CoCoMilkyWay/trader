{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23cac63e",
   "metadata": {},
   "source": [
    "### What are we missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e0d84",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px\">\n",
    "\n",
    "| **Aspect** | **MLE (Maximum Likelihood Estimation) 最大似然估计** | **MAP (Maximum A Posteriori Estimation) 最大后验估计** |\n",
    "|------------|------------------------------------------|--------------------------------------------|\n",
    "| **Objective** | Estimate parameter $\\theta$ that maximizes the likelihood of the observed data. | Estimate parameter $\\theta$ that maximizes the posterior probability given the data. |\n",
    "| **Optimization Goal** | $\\displaystyle \\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} P(D \\mid \\theta)$ | $\\displaystyle \\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(\\theta \\mid D)$ |\n",
    "| **Formula Derivation** | Maximize the likelihood: <br> $\\displaystyle \\mathcal{L}(\\theta) = \\prod_{i=1}^{n} P(x_i \\mid \\theta)$ <br> Take the log: <br> $\\displaystyle \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^{n} \\log P(x_i \\mid \\theta)$ <br> Then: <br> $\\displaystyle \\hat{\\theta}_{\\text{MLE}} = \\arg\\max_{\\theta} \\log P(D \\mid \\theta)$ | Use Bayes’ Theorem: <br> $\\displaystyle P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}$ <br> Ignore constant $P(D)$: <br> $\\displaystyle \\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(D \\mid \\theta) P(\\theta)$ <br> or log-form: <br> $\\displaystyle \\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\left[ \\log P(D \\mid \\theta) + \\log P(\\theta) \\right]$ |\n",
    "| **Includes Prior?** | ❌ No | ✅ Yes |\n",
    "| **Sensitivity to Prior** | Not sensitive (no prior used) | Sensitive to prior choice |\n",
    "| **Overfitting Risk** | Higher, especially for small data | Lower, prior acts as regularizer |\n",
    "| **Asymptotic Behavior** | As $n \\to \\infty$, MLE is consistent | As $n \\to \\infty$, MAP $\\to$ MLE |\n",
    "| **Computational Complexity** | Lower (no prior term) | Higher (includes prior) |\n",
    "| **Interpretation** | Frequentist — parameters are fixed | Bayesian — parameters are random variables |\n",
    "| **Uniform Prior Case** | MLE = MAP | Yes, if $P(\\theta)$ is uniform |\n",
    "| **Regularization View** | No regularization | Prior acts like regularization <br> Gaussian prior $\\Rightarrow L_2$ <br> Laplace prior $\\Rightarrow L_1$ |\n",
    "| **Example: Gaussian Likelihood** | $x_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ <br> $\\displaystyle \\hat{\\mu}_{\\text{MLE}} = \\frac{1}{n} \\sum x_i$ | Prior: $\\mu \\sim \\mathcal{N}(\\mu_0, \\tau^2)$ <br> $\\displaystyle \\hat{\\mu}_{\\text{MAP}} = \\frac{n\\sigma^{-2}}{n\\sigma^{-2} + \\tau^{-2}} \\bar{x} + \\frac{\\tau^{-2}}{n\\sigma^{-2} + \\tau^{-2}} \\mu_0$ |\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294e93b",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px\">\n",
    "<p>Modeling uncertainty is key to capture sparse signal in low SNR environments.<br>\n",
    "Alternatives like voting/agreement rate in Random-Forest–like models are not good enough.<br>\n",
    "Probabilistic Programming models provide more principled uncertainty estimation (❌ not prefect though).</p>\n",
    "\n",
    "$\n",
    "\\underbrace{P(w \\mid D)}_{\\text{posterior}} \n",
    "= \\frac{\n",
    "    \\overbrace{P(D \\mid w)}^{\\text{likelihood}} \\cdot \n",
    "    \\overbrace{P(w)}^{\\text{prior}}\n",
    "}{\n",
    "    \\underbrace{P(D)}_{\\text{evidence}}\n",
    "}\n",
    "$\n",
    "\n",
    "<p>General Form of Prior P(w):</p>\n",
    "\n",
    "$\n",
    "P(w) := \\mathbb{E}_{x, t, \\theta, \\varepsilon} \\left[ P(w \\mid x, t, \\theta, \\varepsilon) \\right] = \\int P(w \\mid x, t, \\theta, \\varepsilon) \\, P(x) P(t) P(\\theta) P(\\varepsilon) \\, dx \\, dt \\, d\\theta \\, d\\varepsilon \\ \\text{(weighted average)}\\\\\n",
    "P(w \\mid x, t, \\theta, \\varepsilon) = \\mathcal{F}(x, t, \\theta, \\varepsilon) \\approx \\underbrace{P(w \\mid \\theta)}_{\n",
    "\\begin{array}{c}\n",
    "    \\text{usually in (Deep)ProbProg models (e.g. BNN)}\\\\\n",
    "    \\text{assume static, noise-free and feature-independent}\n",
    "\\end{array}\n",
    "}\n",
    "$\n",
    "\n",
    "<p>Conditioning terms:</p>\n",
    "\n",
    "- $x$: input/context — adapts prior to input\n",
    "- $t$: time — allows temporal dynamics\n",
    "- $\\theta$: hyperparameters — controls prior structure\n",
    "- $\\varepsilon$: noise — models stochasticity\n",
    "\n",
    "\n",
    "## Bayesian Neural Network(BNN) of Deep Probabilistic Programming as an approximated implementation of MAP Estimation\n",
    "\n",
    "<p>Assuming:</p>\n",
    "\n",
    "- Likelihood $P(D \\mid w) = P(y_{1:N} \\mid x_{1:N}, w) \\overbrace{=}^{\n",
    "    \\begin{array}{c}\n",
    "        \\text{autoregressive}\\\\\n",
    "        \\text{decomposition}\n",
    "    \\end{array}\n",
    "}\\\\\n",
    "\\prod_{i=1}^N P(y_i \\mid y_{<i}, x_{\\le i}, w) \\overbrace{\\approx}^{\\text{model}}\\\\\n",
    "\\prod_{i=1}^N \\mathcal{N}(y_i; f_w(y_{<i}, x_{\\le i}), \\sigma^2)\\\\\n",
    "\\Rightarrow \\log P(D \\mid w) = \\sum_{i=1}^N \\log P(y_i \\mid y_{<i}, x_{\\le i}, w) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f_w(y_{\\le i}, x_{\\le i}))^2 + \\text{const}$\n",
    "\n",
    "- Prior $P(w) = \\mathbb{E}_{x,t,\\varepsilon}[P(w \\mid x, t, \\theta, \\varepsilon)] \\overbrace{\\approx}^{\\text{model}} P(w \\mid \\theta) = \\mathcal{N}(w; 0, \\tau^2 I)\\\\\n",
    "\\Rightarrow \\log P(w) = -\\frac{1}{2\\tau^2} \\|w\\|^2 + \\text{const}$\n",
    "\n",
    "\n",
    "<p>The MAP objective becomes:</p>\n",
    "\n",
    "$\n",
    "w^* = \\arg\\max_w P(w \\mid D) = \\arg\\max_w P(D \\mid w) \\cdot P(w) = \\arg\\max_w log P(D \\mid w) + log P(w)\n",
    "= \\arg\\max_w \\left( \\sum_{i=1}^N \\log P(y_i \\mid y_{<i}, x_{\\le i}, w) + \\log P(w) \\right)\\\\\n",
    "= \\arg\\min_w \\left( - \\sum_{i=1}^N \\log P(y_i \\mid y_{<i}, x_{\\le i}, w) - \\log P(w) \\right)\n",
    "$\n",
    "\n",
    "- Note that because **MLE** and **MAP** use argmax/argmin to formulate the problem, they are **point estimates**, but it can be used to retrieve approximation of the full distribution\n",
    "\n",
    "<p>with Gaussian assumption:</p>\n",
    "\n",
    "$\n",
    "= \\arg\\min_w \\left(\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f_w(y_{<i}, x_{\\le i}))^2 + \\frac{1}{2\\tau^2} \\|w\\|^2 \\right)\n",
    "$\n",
    "\n",
    "<p>with i.i.d. assumption:</p>\n",
    "\n",
    "$\n",
    "= \\arg\\min_w \\left(\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f_w(x_i))^2 + \\frac{1}{2\\tau^2} \\|w\\|^2 \\right)\n",
    "$\n",
    "\n",
    "<p>Which is equivalent to:</p>\n",
    "\n",
    "$\n",
    "\\text{Loss}(w) = \\text{MSE loss} + \\text{L2 regularization} = \\sum_{i=1}^N \\left( y_i - f_w(x_i) \\right)^2 + \\lambda \\cdot \\|w\\|^2, \\quad \\text{with } \\lambda = \\frac{\\sigma^2}{\\tau^2}\n",
    "$\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b82af",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px\">\n",
    "\n",
    "## All BNN Training Methods (forward → loss compute → backward → weight update):\n",
    "\n",
    "| **Method**            | **Posterior Type**                           | **Inference Type**     | **Assumptions Made**                                                                                                                                          | **Uncertainty Quality** | **Scalability** | **Compute**    | **Packages**                           | **References**                                       | **Assumptions Handled By Model?**                                        | **Exact Posterior in Limit?**         | **Overfitting Risk**                                                         |\n",
    "|----------------------|-----------------------------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------|------------------|----------------|----------------------------------------|------------------------------------------------------|---------------------------------------------------------------------------|----------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **Bayes by Backprop** | Mean-field Gaussian                          | Variational              | Weights are independent; posterior is fully factorized Gaussian                                                                                               | Medium                   | High             | Low            | Pyro, Blitz-BNN, Bayesian-Torch        | Blundell et al., 2015                               | ❌ Posterior factorization not modeled explicitly                           | ❌ No                                 | **High** — limited posterior capacity encourages under-regularized solutions |\n",
    "| **Flipout**           | Factorized Gaussian (decorrelated noise)     | Variational              | Weights are independent; Gaussian posterior; noise decorrelated across examples                                                                               | Medium+                  | High             | Low–Medium     | TensorFlow Probability                 | Wen et al., 2018                                  | ⚠️ Decorrelated noise reduces gradient variance, not structural assumptions  | ❌ No                                 | **Medium–High** — slightly improved over BBP                                 |\n",
    "| **SGLD**              | Sampled Posterior                            | MCMC                     | Langevin dynamics without MH; uses minibatches; assumes step size ε→0; assumes stochastic gradient noise does not dominate Langevin noise    | High                     | High             | Medium         | PyTorch-Bayes, Emukit                  | Welling & Teh, 2011                                | ⚠️ Approximate posterior unless step size decays and noise is unbiased      | ⚠️ Only asymptotically (ε→0)           | **Medium** — implicit noise helps, but bias may hurt posterior accuracy     |\n",
    "| **pSGLD**             | Preconditioned Posterior Samples             | MCMC                     | Same as SGLD; assumes curvature can be estimated online to scale gradients; assumes stability in adaptive noise statistics                                     | High+                    | High             | Medium+        | TFP (custom), Pyro (custom)            | Li et al., 2016                                    | ⚠️ Curvature modeled adaptively; still asymptotic correctness only           | ⚠️ Only asymptotically (ε→0)           | **Low–Medium** — better exploration helps avoid local minima                |\n",
    "| **HMC**               | Exact Posterior                              | MCMC                     | No approximation; assumes smooth potential energy; full data gradients; no subsampling allowed                                                                | Very High                | Low              | Very High      | Stan, PyMC3, TF Probability            | Neal, 2011                                         | ✅ Fully nonparametric; assumptions hold for smooth models                  | ✅ Yes (only asymptotically on sample -> ∞)    | **Low** — proper posterior prevents overfitting                             |\n",
    "| **Laplace Approx.**   | Gaussian around MAP                          | Deterministic            | Posterior is Gaussian near MAP; assumes curvature (Hessian) captures uncertainty                                                                              | Low–Medium               | High             | Low (Post-hoc) | LaplaceTorch, GPyTorch                 | MacKay, 1992                                       | ❌ Strong Gaussianity assumption near MAP                                   | ❌ No                                 | **High** — narrow posterior underestimates uncertainty                      |\n",
    "| **Expectation Prop.** | Moment-matched Gaussian                      | Deterministic Approx.    | Approximates each likelihood term with Gaussian; moment-matching used to update posterior                                                                     | Medium–High              | Low              | High           | Edward1, GPy                           | Minka, 2001                                        | ❌ Still assumes Gaussian factors; better fit than mean-field                | ❌ No                                 | **Medium** — improved fit, but still approximate                            |\n",
    "| **Functional BNN**    | Posterior over Functions (not weights)       | Hybrid (VI + GP-style)   | Prior and posterior over output functions; architecture defines function space distribution                                                                   | Very High                | Low              | Very High      | Neural Processes, GPJax, Functorch     | Garnelo et al., 2018; Rasmussen & Williams, 2006  | ✅ Function-space inference avoids parametric assumptions in weights         | ✅ Yes                                | **Low** — function-level regularization is strong                           |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff235a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=LlzVlqVzeD8&list=PLHSMzCAQRltMGNQ9MxE7YBV87N0btrlUo&ab_channel=PyData\n",
    "# https://www.youtube.com/watch?v=KhAUfqhLakw&list=PLHSMzCAQRltMGNQ9MxE7YBV87N0btrlUo&index=2&ab_channel=Enthought\n",
    "# https://www.youtube.com/watch?v=i5PEMt21dO8&list=PLBjSxdPpAJGz-zSjO1Lpkc-0ibLTcz2o9&ab_channel=SMILES-SummerSchoolofMachineLearningatSK\n",
    "\n",
    "# | Feature / Library                | **Pyro**                  | **Blitz-Bayesian-PyTorch**  | **Bayesian-Torch**            | **PyMC**                       | **NumPyro**               | **TensorFlow Probability (TFP)**        |\n",
    "# | -------------------------------- | ------------------------- | --------------------------- | ----------------------------- | ------------------------------ | ------------------------- | --------------------------------------- |\n",
    "# | **Backend**                      | PyTorch                   | PyTorch                     | PyTorch                       | Aesara / JAX                   | JAX                       | TensorFlow                              |\n",
    "# | **Type**                         | Probabilistic Programming | Lightweight BNN             | Modular Bayesian Layers       | Probabilistic Programming      | Probabilistic Programming | Probabilistic Programming + Layers      |\n",
    "# | **Inference Methods**            | SVI, HMC, NUTS            | Variational Inference       | VI, MC Dropout                | NUTS, HMC, ADVI                | NUTS, HMC, SVI            | HMC, VI, EM                             |\n",
    "# | **BNN Support**                  | ✔️ Custom BNNs            | ✔️ Easy BNNs via decorators| ✔️ Deep BNNs & drop-in layers | ⚠️ Basic BNN support          | ⚠️ Some support (manual)  | ✔️ Keras BNN Layers                    |\n",
    "# | **Ease of Use**                  | Medium                    | Easy                        | Medium                        | Easy                           | Medium                    | Medium                                  |\n",
    "# | **Deep Learning Scale**          | ✔️ Yes                    | ✔️ Yes                     | ✔️ Yes                        | ❌ Limited                    | ⚠️ Limited GPU support    | ✔️ Yes (via TensorFlow)                |\n",
    "# | **GPU Acceleration**             | ✔️ Yes                    | ✔️ Yes                     | ✔️ Yes                        | ⚠️ Limited (JAX backend only) | ✔️ JAX (fast!)            | ✔️ TensorFlow                          |\n",
    "# | **Good for Probabilistic Logic** | ✔️ Yes                    | ❌                         | ❌                            | ✔️ Yes                        | ✔️ Yes                    | ✔️ Yes                                 |\n",
    "# | **Learning Curve**               | Steep                     | Low                         | Medium                        | Medium                         | Medium                    | Medium                                  |\n",
    "# | **Community & Maturity**         | Large (Uber, academic)    | Small                       | Medium                        | Large & mature                 | Growing fast (Google)     | Large (Google)                          |\n",
    "# | **Best Use Case**                | Custom probabilistic BNNs | Quick, practical BNNs       | Plug-and-play BNNs            | Statistical models, small BNNs | Fast HMC/VI for research  | Keras-style probabilistic deep learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cc71e",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px\">\n",
    "\n",
    "---\n",
    "### MCMC-pSGLD: (Markov Chain Monte Carlo - preconditioned Stochastic Gradient Langevin Dynamics)\n",
    "#### Forward Pass:\n",
    "- **Monte Carlo**: most MAP models are generative (modeling the approximated real joint distribution), including this one<br>\n",
    "  we cannot simply use mean from each parameter node to calculate the output<br>\n",
    "  A single prediction requires multiple forward passes (samples) from our trained model $p(\\theta \\mid \\mathcal{D}_{\\text{train}})$,<br>\n",
    "  using a random sampling method that matches the true posterior distribution (high-dimensional, intractable, unnormalized)<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underbrace{p(y_{\\text{test}} \\mid X_{\\text{test}}, \\mathcal{D}_{\\text{train}})}_{\\textbf{Bayesian Prediction}}\n",
    "&= \\int \n",
    "\\underbrace{p(y_{\\text{test}} \\mid X_{\\text{test}}, \\theta)}_{\\textbf{Likelihood (model output sample)}}\n",
    "\\cdot \n",
    "\\underbrace{p(\\theta \\mid \\mathcal{D}_{\\text{train}})}_{\\textbf{True Posterior}}\n",
    "\\, d\\theta \n",
    "\\\\[1.2em]\n",
    "&= \\underbrace{\\mathbb{E}_{\\theta \\sim p(\\theta \\mid \\mathcal{D}_{\\text{train}})} \\left[ p(y_{\\text{test}} \\mid X_{\\text{test}}, \\theta) \\right]}_{\\textbf{Expectation over Posterior}}\n",
    "\\\\[1.2em]\n",
    "&\\approx \\underbrace{\\frac{1}{T} \\sum_{i=1}^T p(y_{\\text{test}} \\mid X_{\\text{test}}, \\theta^{(i)})}_{\\textbf{Monte Carlo Estimate}}\n",
    "\\quad \\text{where } \\theta^{(i)} \\sim p(\\theta \\mid \\mathcal{D}_{\\text{train}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Loss Computation:\n",
    "- Refer to previous section (without Gaussian/i.i.d. assumptions)\n",
    "\n",
    "- The MAP point estimates contains loss definition: <br>\n",
    "    $ w^* = \\arg\\min_w \\left( - \\sum_{i=1}^N \\log P(y_i \\mid y_{<i}, x_{\\le i}, w) - \\log P(w) \\right) $\n",
    "\n",
    "#### Backward Pass:\n",
    "- we try to evaluate weight through the joint distribution of posterior, not just MAP point estimate\n",
    "- instead of computing gradient of loss (negative log-likelihood), here we compute gradient of log-posterior\n",
    "- the \"Training\"(posterior sampling) happens after all data is present, we use an algorithm to explore parameter space(state space of MC, support for posterior distribution) to find the most 'fitted'(approximate) posterior distribution(joint) over many iterations (iteration (time in Markov Chain in latent space) != sample (time in sequential samples))\n",
    "- **Markov Chain**: the True Posterior $p(\\theta \\mid \\mathcal{D}_{\\text{train}})$ can be approximated as stationary distribution of a Markov Chain(discrete) as number of steps goes to infinity (the state space of this Markov process is also the support of the true posterior, which is $\\theta \\in \\mathbb{R}^D$)<br>\n",
    "    if we assume:\n",
    "    - **Ergodicity**: The chain forgets its starting point.\n",
    "        - $\\forall \\theta, \\theta', \\exists t \\in \\mathbb{N} \\text{ such that } K^t(\\theta' \\mid \\theta) > 0$\n",
    "        - Ergodicity ⇐ Aperiodicity + Irreducibility\n",
    "            - **Aperiodicity**: No cyclic pattern in transitions\n",
    "            - **Irreducibility**: Every state is reachable from every other state in finite steps\n",
    "    - **Time-homogeneity**: Transition probabilities $K$ are fixed over time\n",
    "    - Target Invariance via **Detailed Balance**:\n",
    "        - $p(\\theta \\mid \\mathcal{D}) K(\\theta' \\mid \\theta) = p(\\theta' \\mid \\mathcal{D}) K(\\theta \\mid \\theta') \\quad \\text{for all } \\theta, \\theta'$\n",
    "        - this is actually Microscopic symmetry in parameter space: forward flow = backward flow\n",
    "        - implies **Stationarity** if K is chosen correctly regarding D: distribution is fixed\n",
    "            - $p(\\theta' \\mid \\mathcal{D}) = \\int_\\Theta K(\\theta' \\mid \\theta) p(\\theta \\mid \\mathcal{D}) \\, d\\theta \\quad \\text{for all } \\theta'$\n",
    "    - ✅ all previous assumptions:\n",
    "        - can always be constructed(exist) via K regardless of D (training data)\n",
    "        - However, in some assumptions, K is also dependent on D, which means K needs to be carefully constructed\n",
    "\n",
    "- Let $\\{\\theta_t\\}_{t=0}^\\infty$ be a Markov chain\n",
    "- $A$ be a measurable region of parameter space (e.g. i-th component of θ>0.5, accuracy(θ)>90%, etc.)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\lim_{t \\to \\infty} \\mathbb{P}(\\theta_t \\in A)\n",
    "&= \\lim_{t \\to \\infty}\n",
    "\\underbrace{\n",
    "\\int_{\\Theta} \\cdots \\int_{\\Theta}\n",
    "}_{t \\text{ nested integrals}} \\;\n",
    "\\underbrace{K(\\theta_t \\mid \\theta_{t-1})}_{\\text{transition kernel}} \\cdots K(\\theta_1 \\mid \\theta_0)\n",
    "\\underbrace{\\mu_0(\\theta_0)}_{\\text{initial distribution}} \\,\n",
    "\\mathbf{1}_A(\\theta_t)\n",
    "\\; d\\theta_0 \\cdots d\\theta_t\n",
    "\\\\[2ex]\n",
    "&\\quad \\textcolor{gray}{\\text{// Expand marginal probability of } \\theta_t \\in A \\text{ via the full joint chain law: } \\mu_0 \\cdot K \\cdots K}\n",
    "\\\\[2ex]\n",
    "\n",
    "&= \\lim_{t \\to \\infty}\n",
    "\\int_A\n",
    "\\left(\n",
    "\\int_{\\Theta} \\cdots \\int_{\\Theta}\n",
    "K(\\theta_t \\mid \\theta_{t-1}) \\cdots K(\\theta_1 \\mid \\theta_0)\n",
    "\\mu_0(\\theta_0)\n",
    "\\; d\\theta_0 \\cdots d\\theta_{t-1}\n",
    "\\right)\n",
    "d\\theta_t\n",
    "\\\\[2ex]\n",
    "&\\quad \\textcolor{gray}{\\text{// Pull indicator } \\mathbf{1}_A(\\theta_t) \\text{ outside as domain of outermost integral becomes } A}\n",
    "\\\\[2ex]\n",
    "\n",
    "&=\n",
    "\\int_A\n",
    "\\left(\n",
    "\\lim_{t \\to \\infty}\n",
    "(K^t \\mu_0)(\\theta_t)\n",
    "\\right)\n",
    "d\\theta_t\n",
    "\\\\[2ex]\n",
    "&\\quad \\textcolor{gray}{\\text{// Recognize the nested integral as repeated application of the Markov operator: } K^t \\mu_0}\n",
    "\\\\[2ex]\n",
    "\n",
    "&=\n",
    "\\int_A p(\\theta \\mid \\mathcal{D}) \\, d\\theta\n",
    "\\\\[2ex]\n",
    "&\\quad \\textcolor{gray}{\n",
    "\\text{// By ergodic theorem: if } K \\text{ is ergodic and satisfies detailed balance w.r.t. } p(\\theta \\mid \\mathcal{D})\n",
    "\\Rightarrow \\lim_{t \\to \\infty} K^t \\mu_0 = p(\\theta \\mid \\mathcal{D}) \\text{ in distribution}\n",
    "}\n",
    "\\\\[2ex]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\theta \\mid \\mathcal{D}) \n",
    "&= \\frac{p(\\mathcal{D}, \\theta)}{p(\\mathcal{D})}\n",
    "= \\frac{p(\\mathcal{D} \\mid \\theta)\\, p(\\theta)}{\\int_{\\mathbb{R}^d} p(\\mathcal{D} \\mid \\vartheta)\\, p(\\vartheta)\\, d\\vartheta}\n",
    "= \\frac{1}{\\int_{\\mathbb{R}^d} p(\\mathcal{D} \\mid \\vartheta)\\, p(\\vartheta)\\, d\\vartheta} \\cdot p(\\mathcal{D} \\mid \\theta)\\, p(\\theta) \\\\[10pt]\n",
    "\n",
    "&= \\frac{1}{Z} \\cdot p(\\mathcal{D} \\mid \\theta)\\, p(\\theta)\n",
    "= \\frac{1}{Z} \\cdot \\exp\\left( \\log p(\\mathcal{D} \\mid \\theta) + \\log p(\\theta) \\right)\n",
    "= \\frac{1}{Z} \\cdot \\exp\\left( -[-\\log p(\\mathcal{D} \\mid \\theta) - \\log p(\\theta)] \\right) \\\\[10pt]\n",
    "\n",
    "&= \\underbrace{\\frac{1}{Z} \\cdot \\exp\\left( -U(\\theta) \\right)}_{\\text{Gibbs (Boltzmann) form}}\n",
    "\\qquad \\text{where:} \\quad\n",
    "\\begin{cases}\n",
    "\\text{Potential Energy}: U(\\theta) := -\\log p(\\mathcal{D} \\mid \\theta) - \\log p(\\theta) >= 0 \\quad \\text{(both non-negative)}\\\\[4pt]\n",
    "\\begin{array}{c}\n",
    "    \\text{Partition Function}\\\\\n",
    "    \\text{Normalization Constant}\n",
    "\\end{array}\n",
    ": Z := \\int_{\\mathbb{R}^d} \\exp(-U(\\vartheta))\\, d\\vartheta\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- There are many models in physics(statistical mechanics, thermo/quantum dynamics) that relates potential energy field to probability distribution\n",
    "- the Posterior distribution can be written as a Potential-Energy-based Particle-Diffusion model in latent-space as well\n",
    "    - this is only to help us intuitively understand the distribution, the physical analogy is not necessary\n",
    "    - we need to find a Markov Chain that:\n",
    "        - has transition kernel that this potential distribution is one of its stationary solutions\n",
    "        - satisfy previous assumptions\n",
    "    - some properties that we want/realized:\n",
    "        - the lower the energy, the higher the probability (posterior in latent space)\n",
    "        - the shape of posterior is complex (multiple modes/peaks) in latent space\n",
    "        - the kernel needs to work as a compass to guide the transition towards nearest (depends on space type) local maxima of probability or minima of potential energy\n",
    "            - only then, the Markov Chain can stay longer in the more probable region to form the correct distribution\n",
    "        - also it needs to have some random/stochastic/drifting/diffusion properties to help explore the whole latent space (guarantee some of previous assumptions)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\underbrace{d\\theta_t = -\\nabla U(\\theta_t)\\,dt + \\sqrt{2}\\,dW_t}_{\\substack{\\text{Over-Damped Langevin dynamics SDE:} \\\\ \\text{gradient drift + Gaussian noise}}}\n",
    "\\\\[1.2em]\n",
    "&\\Rightarrow \n",
    "\\underbrace{\n",
    "\\frac{\\partial \\rho(\\theta, t)}{\\partial t} = \\nabla \\cdot \\left( \\nabla U(\\theta)\\, \\rho(\\theta, t) + \\nabla \\rho(\\theta, t) \\right)\n",
    "}_{\\substack{\\text{Fokker–Planck equation:} \\\\ \\text{evolution of density}}}\n",
    "\\\\[1.5em]\n",
    "&\\Rightarrow \n",
    "\\nabla \\cdot \\left( \\nabla U(\\theta)\\, \\rho(\\theta) + \\nabla \\rho(\\theta) \\right)\n",
    "= \\nabla \\cdot \\left( \\nabla U(\\theta) \\cdot \\tfrac{1}{Z} e^{-U(\\theta)} + \\nabla \\left( \\tfrac{1}{Z} e^{-U(\\theta)} \\right) \\right)\n",
    "= \\nabla \\cdot \\left( \\tfrac{1}{Z} e^{-U(\\theta)} \\nabla U(\\theta) - \\tfrac{1}{Z} e^{-U(\\theta)} \\nabla U(\\theta) \\right)\n",
    "= \\nabla \\cdot (0) = 0\n",
    "\\\\[1.5em]\n",
    "&\\Leftrightarrow \n",
    "\\rho(\\theta) = \\tfrac{1}{Z} \\exp(-U(\\theta)) \\;\\text{is stationary under Langevin dynamics}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Time-homogeneous: $U(\\theta)$ is fixed for a given posterior\n",
    "- Ergodicity: (the Fokker-Planck equation)\n",
    "    - covers full support $\\theta \\in \\mathbb{R}^D$\n",
    "    - each state is reachable due to diffusion (Brownian term)\n",
    "    - No periodicity due to stochasticity\n",
    "- Detailed Balance (Microscopic Reversibility):\n",
    "    - The Fokker–Planck operator is self-adjoint in the weighted space $L^2(p^*)$\n",
    "    - The generator of Langevin dynamics is reversible with respect to $p^*(\\theta)$\n",
    "\n",
    "- Alternatively:\n",
    "    - instead of Over-Damped Langevin: Potential + Diffusion(Noise) (friction high enough just to remove momentum)\n",
    "    - we can have:\n",
    "        - Under-Damped Langevin: Potential + Diffusion + Kinetic(Momentum)\n",
    "        - Hamiltonian SDE: Potential + Kinetic (energy perfectly conserved)\n",
    "        - Noisy Hamiltonian SDE: Potential + Kinetic + Diffusion (energy not perfectly conserved)\n",
    "    - Kinetic => better preserve energy => inertial exploration => better long-range exploration\n",
    "\n",
    "$$\n",
    "H(\\theta, p) = \\underbrace{-\\log p(\\theta \\mid \\mathcal{D})}_{\\text{Potential } U(\\theta)} + \\underbrace{\\frac{1}{2} p^T M^{-1} p}_{\\text{Kinetic } K(p)}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Weight Update:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\underbrace{d\\theta_t = -\\nabla_\\theta U(\\theta_t)\\,dt + \\sqrt{2}\\,dW_t}_{\\text{Langevin SDE (Itô)}} \n",
    "= \n",
    "\\underbrace{\\theta_{t+1} = \\theta_t - \\epsilon \\nabla_\\theta U(\\theta_t) + \\sqrt{2\\epsilon} \\, \\xi_t}_{\\text{Euler–Maruyama discretization} \\quad \\xi_t \\sim \\mathcal{N}(0, I)} \n",
    "\\Rightarrow\n",
    "\\underbrace{q(\\theta'|\\theta_t) = \\mathcal{N}\\left(\\theta' \\mid \\theta_t - \\epsilon \\nabla_\\theta U(\\theta_t), 2\\epsilon I\\right)}_{\\text{Proposal distribution}} \n",
    "\\\\\n",
    "&\\Rightarrow \n",
    "\\underbrace{\n",
    "\\alpha(\\theta_t, \\theta') = \\min\\left(1, \n",
    "\\frac{\n",
    "e^{-U(\\theta')} \\cdot \n",
    "\\exp\\left(-\\frac{1}{4\\epsilon} \\|\\theta_t - \\theta' + \\epsilon \\nabla_\\theta U(\\theta')\\|^2 \\right)\n",
    "}{\n",
    "e^{-U(\\theta_t)} \\cdot \n",
    "\\exp\\left(-\\frac{1}{4\\epsilon} \\|\\theta' - \\theta_t + \\epsilon \\nabla_\\theta U(\\theta_t)\\|^2 \\right)\n",
    "}\n",
    "\\right)}_{\\text{Metropolis–Hastings acceptance prob. (Detailed Balance)}} \n",
    "\\\\\n",
    "&\\Rightarrow \n",
    "\\underbrace{\n",
    "\\theta_{t+1} =\n",
    "\\begin{cases}\n",
    "\\theta', & \\text{with probability } \\alpha(\\theta_t, \\theta') \\\\\n",
    "\\theta_t, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "}_{\\text{MALA (Metropolis-Adjusted Langevin Algorithm)}} \n",
    "= \n",
    "\\underbrace{\n",
    "\\theta_{t+1} =\n",
    "\\begin{cases}\n",
    "\\theta_t - \\epsilon \\nabla_\\theta U(\\theta_t) + \\sqrt{2\\epsilon}\\,\\xi_t, & \\text{if accepted} \\\\\n",
    "\\theta_t, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "}_{\\text{SGLD with MH correction: samples from } \\pi(\\theta) \\propto e^{-U(\\theta)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- in pSGLD:\n",
    "    - we use mini-batches and preconditioning to approximate and skip Metropolis-Hastings correction for bias introduced in Euler-Maruyama discretization\n",
    "        - but as long as step size $\\epsilon_t \\to 0$ slowly and the preconditioner stabilizes, the sampling bias from ignoring MH can be minimized (Li et al., 2016)\n",
    "    - pSGLD has better uncertainty estimation than SGLD because it incorporates local curvature information (via preconditioning)\n",
    "        - the injected noise and gradient step are scaled according to local curvature, rather than isotropic noise in standard SGLD\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "d\\theta_t &= \\theta_t - \\epsilon \\left\\{ \\underbrace{ - \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log p(y_i|x_i, \\theta_t) - \\nabla_\\theta \\log p(\\theta_t) }_{ \\nabla_\\theta U(\\theta_t) } \\right\\} + \\sqrt{2\\epsilon} \\, \\xi_t \\\\\n",
    "&\\xRightarrow{\\text{minibatch approx.}} \\theta_{t+1} = \\theta_t - \\epsilon \\left\\{ \\underbrace{ - \\frac{N}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\nabla_\\theta \\log p(y_i|x_i, \\theta_t) - \\nabla_\\theta \\log p(\\theta_t) }_{ \\hat{\\nabla}_\\theta U(\\theta_t) \\quad \\text{stochastic gradient estimate from mini-batch}} \\right\\} + \\sqrt{2\\epsilon} \\, \\xi_t \\\\\n",
    "&\\xRightarrow{\\text{preconditioned}} \\theta_{t+1} = \\theta_t - \\epsilon \\cdot \\frac{1}{2} \\underbrace{G(\\theta_t)}_{\\text{diagonal preconditioning matrix (RMSprop-style)}} \\cdot \\hat{\\nabla}_\\theta U(\\theta_t) + \\underbrace{\\mathcal{N}(0, \\epsilon G(\\theta_t))}_{\\text{noise}} \\\\\n",
    "&\\Rightarrow \\theta_{t+1} = \\theta_t - \\frac{\\epsilon}{2} G(\\theta_t) \\hat{\\nabla}_\\theta U(\\theta_t) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\epsilon G(\\theta_t))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2ede5",
   "metadata": {},
   "source": [
    "<div style=\"font-size:14px\">\n",
    "\n",
    "---\n",
    "### VI-Flipout: (Variational Inference - Flipout)\n",
    "\n",
    "<span style=\"color:yellow\">\n",
    "Warning:\n",
    "\n",
    "- in Variational Inference(VI), by forcing each of neural network’s parameter to marginally live inside a fixed, pre-chosen distributional family (e.g., Gaussians):\n",
    "    - No multi-modal behavior\n",
    "    - No skewness\n",
    "    - No heavy tails\n",
    "    - No nonlinear dependencies between weights\n",
    "    - which violates the bare-minimum to model the true posterior (high-dimensional, highly-complex, unnormalized, intractable)\n",
    "\n",
    "- This has far-reaching consequences beyond posterior approximation, it leaks into the model’s functional behavior:\n",
    "    - by imposing strong geometric constraints on the latent space, it may group together very different functions (far apart in function/feature space)\n",
    "        -  this poses a very serious threat when randomly sampling using it as a generative model (unnatural in-between regions)\n",
    "    - in some scenarios, it is possible to act as a regularizer that improves generalization — even if it’s fundamentally incorrect\n",
    "\n",
    "- in MCMC, it preserves probability(true posterior), also not geometry(local latent space <-> local feature space):\n",
    "    - interpolation: it also cannot make sure that nearby points in latent space correspond to similar outputs in data/feature space\n",
    "    - geometry-aware models include:\n",
    "        - Autoencoder-based models\n",
    "        - Normalizing Flows\n",
    "        - Diffusion Models\n",
    "        - Energy-Based Models + Score-Based Learning\n",
    "        - Neural ODEs & Continuous Normalizing Flows\n",
    "        - Metric Learning / Contrastive Learning\n",
    "\n",
    "- even though there is no mathematical guarantee, in reality, basic NN arch still offer some protection that features are still mostly \"continuous\", although with high variance, in latent space:\n",
    "    - Gradient descent (or pSGLD) keeps weight updates small, most of the time\n",
    "    - you stay in a \"smooth\" part of the network function space, most of the time\n",
    "    - smooth activations reduce the chance of sharp transitions\n",
    "    - gradient-based optimization encourages local stability\n",
    "    - **DO NOT take these for granted**\n",
    "\n",
    "</span> \n",
    "\n",
    "Let a **fully connected layer** have weight matrix $\\mathbf{W} \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}$. In Flipout:\n",
    "\n",
    "* Variational posterior:\n",
    "\n",
    "  $$\n",
    "  q(\\mathbf{W}) = \\mathcal{N}(\\mu, \\sigma^2)\n",
    "  \\quad \\text{with reparam:} \\quad\n",
    "  \\mathbf{W} = \\mu + \\sigma \\odot \\epsilon\n",
    "  \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "  $$\n",
    "\n",
    "* Flipout perturbs each weight **per sample** using:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{W}^{(i)} = \\mu + (\\sigma \\odot \\epsilon) \\cdot r^{(i)} s^{(i)\\top}\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "\n",
    "  * $r^{(i)} \\in \\{+1, -1\\}^{d_{\\text{in}}}$, $s^{(i)} \\in \\{+1, -1\\}^{d_{\\text{out}}}$\n",
    "  * $r^{(i)}, s^{(i)} \\sim \\text{Bern}(0.5)$ (Rademacher random variables)\n",
    "  * Note: $\\epsilon \\in \\mathbb{R}^{d_{\\text{out}} \\times d_{\\text{in}}}$ shared across batch\n",
    "\n",
    "#### FORWARD PASS (with Flipout)\n",
    "\n",
    "Let the input batch be $\\mathbf{X} \\in \\mathbb{R}^{B \\times d_{\\text{in}}}$, where $B$ is the batch size. For each sample $i \\in \\{1, \\dots, B\\}$:\n",
    "\n",
    "1. **Shared weight perturbation**:\n",
    "\n",
    "   $$\n",
    "   \\Delta \\mathbf{W} = \\sigma \\odot \\epsilon\n",
    "   $$\n",
    "\n",
    "2. **Pseudo-independent perturbed weights (via Flipout)**:\n",
    "\n",
    "   $$\n",
    "   \\Delta \\mathbf{W}^{(i)} = \\Delta \\mathbf{W} \\odot \\left( s^{(i)} r^{(i)\\top} \\right)\n",
    "   $$\n",
    "\n",
    "3. **Compute output** (for each example $i$):\n",
    "\n",
    "   $$\n",
    "   \\mathbf{y}^{(i)} = \\mathbf{x}^{(i)} \\cdot \\mu^\\top + \\left( \\left( \\mathbf{x}^{(i)} \\odot r^{(i)} \\right) \\cdot \\Delta \\mathbf{W}^\\top \\right) \\odot s^{(i)}\n",
    "   $$\n",
    "\n",
    "4. **Batched matrix form** (Flipout trick):\n",
    "\n",
    "   $$\n",
    "   \\mathbf{Y} = \\mathbf{X} \\mu^\\top + \\left( (\\mathbf{X} \\odot \\mathbf{R}) \\cdot \\Delta \\mathbf{W}^\\top \\right) \\odot \\mathbf{S}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   * $\\mathbf{R}, \\mathbf{S} \\in \\mathbb{R}^{B \\times d}$ are matrices of random signs\n",
    "\n",
    "#### LOSS FUNCTION: Evidence Lower Bound (ELBO)\n",
    "\n",
    "We optimize the **negative ELBO**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VI}}(\\theta) = -\\mathbb{E}_{q(\\mathbf{W})} \\left[ \\log p(\\mathcal{D} \\mid \\mathbf{W}) \\right] + \\text{KL}\\left[ q(\\mathbf{W}) \\,\\|\\, p(\\mathbf{W}) \\right]\n",
    "$$\n",
    "\n",
    "* First term (expected log-likelihood) approximated via Monte Carlo (Flipout samples).\n",
    "* Second term (KL divergence) is analytical for Gaussian:\n",
    "\n",
    "If:\n",
    "\n",
    "* Prior: $p(\\mathbf{W}) = \\mathcal{N}(0, \\sigma_0^2 I)$\n",
    "* Posterior: $q(\\mathbf{W}) = \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{KL}(q \\| p) = \\sum_{j=1}^d \\left[ \\log \\frac{\\sigma_0}{\\sigma_j} + \\frac{\\sigma_j^2 + \\mu_j^2}{2\\sigma_0^2} - \\frac{1}{2} \\right]\n",
    "$$\n",
    "\n",
    "#### BACKWARD PASS\n",
    "\n",
    "We compute gradients of:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{i=1}^B \\log p(y^{(i)} \\mid \\mathbf{x}^{(i)}, \\mathbf{W}^{(i)}) + \\text{KL}(q \\| p)\n",
    "$$\n",
    "\n",
    "Backprop proceeds as:\n",
    "\n",
    "* Gradients flow through Flipout layers via the reparameterization trick.\n",
    "* The key is to ensure that gradients w\\.r.t. $\\mu$ and $\\sigma$ are **unbiased** estimates of the gradient of the ELBO.\n",
    "\n",
    "Let’s derive:\n",
    "\n",
    "- Gradient w\\.r.t. μ\n",
    "\n",
    "$$\n",
    "\\nabla_\\mu \\mathcal{L} \\approx \\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\mathbf{W}^{(i)}} \\log p(y^{(i)} \\mid \\mathbf{x}^{(i)}, \\mathbf{W}^{(i)}) \\cdot \\nabla_\\mu \\mathbf{W}^{(i)} + \\nabla_\\mu \\text{KL}\n",
    "$$\n",
    "\n",
    "But:\n",
    "\n",
    "$$\n",
    "\\nabla_\\mu \\mathbf{W}^{(i)} = I\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\nabla_\\mu \\mathcal{L} \\approx \\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\mathbf{W}^{(i)}} \\log p(y^{(i)} \\mid \\mathbf{x}^{(i)}, \\mathbf{W}^{(i)}) + \\nabla_\\mu \\text{KL}\n",
    "$$\n",
    "\n",
    "- Gradient w\\.r.t. σ\n",
    "\n",
    "$$\n",
    "\\nabla_\\sigma \\mathbf{W}^{(i)} = \\epsilon \\odot (r^{(i)} s^{(i)\\top})\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\nabla_\\sigma \\mathcal{L} \\approx \\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\mathbf{W}^{(i)}} \\log p(y^{(i)} \\mid \\mathbf{x}^{(i)}, \\mathbf{W}^{(i)}) \\odot \\left( \\epsilon \\odot (r^{(i)} s^{(i)\\top}) \\right) + \\nabla_\\sigma \\text{KL}\n",
    "$$\n",
    "\n",
    "- KL Gradient Terms\n",
    "\n",
    "Analytical:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu_j} \\text{KL} = \\frac{\\mu_j}{\\sigma_0^2}, \\quad\n",
    "\\frac{\\partial}{\\partial \\sigma_j} \\text{KL} = -\\frac{1}{\\sigma_j} + \\frac{\\sigma_j}{\\sigma_0^2}\n",
    "$$\n",
    "\n",
    "#### WEIGHT UPDATE (Stochastic Gradient Descent)\n",
    "\n",
    "Let $\\eta$ be the learning rate. We apply gradient descent (or Adam):\n",
    "\n",
    "* $\\mu \\leftarrow \\mu - \\eta \\cdot \\nabla_\\mu \\mathcal{L}$\n",
    "* $\\sigma \\leftarrow \\sigma - \\eta \\cdot \\nabla_\\sigma \\mathcal{L}$\n",
    "\n",
    "To ensure numerical stability, you often optimize $\\rho = \\log \\exp(\\sigma)$ instead.\n",
    "\n",
    "\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cccd7c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n\u001b[32m    125\u001b[39m X_test = np.random.randn(\u001b[32m100\u001b[39m, D)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m mean_pred, std_pred = \u001b[43mpredict_mcmc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMean predictions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, mean_pred)\n\u001b[32m    128\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUncertainty (std) predictions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, std_pred)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mpredict_mcmc\u001b[39m\u001b[34m(model, samples, X_test)\u001b[39m\n\u001b[32m     95\u001b[39m         out = model(X_test).cpu().numpy()\n\u001b[32m     96\u001b[39m         preds.append(out)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m preds = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [num_samples, N_test, output_dim]\u001b[39;00m\n\u001b[32m     98\u001b[39m mean = preds.mean(axis=\u001b[32m0\u001b[39m)\n\u001b[32m     99\u001b[39m std = preds.std(axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuyin.wang\\.pyenv\\pyenv-win\\versions\\3.12.9\\Lib\\site-packages\\numpy\\_core\\shape_base.py:444\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    442\u001b[39m arrays = [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mneed at least one array to stack\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    446\u001b[39m shapes = {arr.shape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) != \u001b[32m1\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ---------- Bayesian Neural Network Definition ----------\n",
    "class BayesianMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(BayesianMLP, self).__init__()\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims)-1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "# ---------- pSGLD Sampler ----------\n",
    "class pSGLD:\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.99, eps=1e-8, weight_decay=1e-2):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        # Initialize running average of squared gradients\n",
    "        self.state = {p: torch.zeros_like(p.data) for p in self.params}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            grad = p.grad.data + self.weight_decay * p.data\n",
    "            v = self.state[p]\n",
    "            v.mul_(self.alpha).addcmul_(grad, grad, value=1 - self.alpha)\n",
    "            precond = 1.0 / (torch.sqrt(v) + self.eps)\n",
    "            noise = torch.randn_like(p.data) * torch.sqrt(self.lr * precond)\n",
    "            p.data.add_(-0.5 * self.lr * precond * grad + noise)\n",
    "\n",
    "# ---------- Training and Sampling ----------\n",
    "def train_mcmc(model, X, y, num_epochs=1000, batch_size=64,\n",
    "               lr=1e-3, alpha=0.99, eps=1e-8, weight_decay=1e-2,\n",
    "               burn_in=500, collect_every=10, num_samples=100,\n",
    "               likelihood_noise=1.0):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    sampler = pSGLD(model.parameters(), lr=lr, alpha=alpha,\n",
    "                    eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "    samples = []\n",
    "    total_steps = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            sampler.params_grad_zero = False\n",
    "            model.zero_grad()\n",
    "            preds = model(xb).squeeze()\n",
    "            # Gaussian likelihood log p(y|f) ~ -(1/(2*sigma^2))*(y-f)^2\n",
    "            neg_log_lik = 0.5 / (likelihood_noise**2) * F.mse_loss(preds, yb, reduction='sum')\n",
    "            neg_log_prior = 0.0\n",
    "            # Gaussian prior N(0, I): -(1/2)*w^2\n",
    "            for p in model.parameters():\n",
    "                neg_log_prior += 0.5 * torch.sum(p**2)\n",
    "            loss = neg_log_lik + neg_log_prior\n",
    "            loss.backward()\n",
    "            sampler.step()\n",
    "            total_steps += 1\n",
    "\n",
    "            # Collect samples after burn-in\n",
    "            if total_steps > burn_in and total_steps % collect_every == 0:\n",
    "                # Deep copy parameters\n",
    "                state = {k: v.clone().cpu() for k, v in model.state_dict().items()}\n",
    "                samples.append(state)\n",
    "                if len(samples) >= num_samples:\n",
    "                    return samples\n",
    "    return samples\n",
    "\n",
    "# ---------- Prediction with Posterior Samples ----------\n",
    "def predict_mcmc(model, samples, X_test):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    X_test = torch.Tensor(X_test).to(device)\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for state in samples:\n",
    "            model.load_state_dict(state)\n",
    "            out = model(X_test).cpu().numpy()\n",
    "            preds.append(out)\n",
    "    preds = np.stack(preds, axis=0)  # [num_samples, N_test, output_dim]\n",
    "    mean = preds.mean(axis=0)\n",
    "    std = preds.std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "# ---------- Usage Example ----------\n",
    "if __name__ == '__main__':\n",
    "    # Generate synthetic data if X, y not provided\n",
    "    N, D = 1000, 10\n",
    "    X = np.random.randn(N, D)\n",
    "    true_w = np.random.randn(D)\n",
    "    y = X.dot(true_w) + 0.1 * np.random.randn(N)\n",
    "\n",
    "    # Initialize model\n",
    "    model = BayesianMLP(input_dim=D, hidden_dims=[50, 50], output_dim=1)\n",
    "\n",
    "    # Train and collect posterior samples\n",
    "    samples = train_mcmc(model, X, y,\n",
    "                         num_epochs=50,\n",
    "                         batch_size=64,\n",
    "                         lr=1e-4,\n",
    "                         weight_decay=1e-4,\n",
    "                         burn_in=1000,\n",
    "                         collect_every=20,\n",
    "                         num_samples=200,\n",
    "                         likelihood_noise=0.1)\n",
    "\n",
    "    # Predict on test set\n",
    "    X_test = np.random.randn(100, D)\n",
    "    mean_pred, std_pred = predict_mcmc(model, samples, X_test)\n",
    "    print(\"Mean predictions:\\n\", mean_pred)\n",
    "    print(\"Uncertainty (std) predictions:\\n\", std_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4048a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: inf\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "\n",
    "# Define the BNN model\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the pSGLD optimizer\n",
    "class PSGLD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta=0.99, epsilon=1e-8):\n",
    "        defaults = dict(lr=lr, beta=beta, epsilon=epsilon)\n",
    "        super(PSGLD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                if 'v' not in state:\n",
    "                    state['v'] = torch.zeros_like(p.data)\n",
    "                v = state['v']\n",
    "                beta = group['beta']\n",
    "                v.mul_(beta).add_((1 - beta) * grad ** 2)\n",
    "                preconditioner = 1 / (v.sqrt() + group['epsilon'])\n",
    "                noise = torch.normal(0, 1, size=p.data.size(), device=p.device)\n",
    "                p.data.add_(-group['lr'] * preconditioner * grad + math.sqrt(2 * group['lr']) * preconditioner * noise)\n",
    "\n",
    "# Generate synthetic data (assuming X and y are not provided)\n",
    "n_samples = 120\n",
    "X = torch.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "y = torch.sin(X) + 0.1 * torch.randn(n_samples, 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "n_train = 100\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# Set up the model\n",
    "input_dim = 1\n",
    "hidden_dim = 50\n",
    "output_dim = 1\n",
    "model = BNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "lr = 0.001\n",
    "beta = 0.99\n",
    "epsilon = 1e-8\n",
    "optimizer = PSGLD(model.parameters(), lr=lr, beta=beta, epsilon=epsilon)\n",
    "\n",
    "# Set up the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Set up the DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training parameters\n",
    "total_epochs = 1000\n",
    "burn_in = 500\n",
    "collect_interval = 10\n",
    "model_samples = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch > burn_in and (epoch - burn_in) % collect_interval == 0:\n",
    "        model_samples.append({k: v.clone() for k, v in model.state_dict().items()})\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "predictions = []\n",
    "for state in model_samples:\n",
    "    model.load_state_dict(state)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "    predictions.append(y_pred)\n",
    "y_pred_mean = torch.mean(torch.stack(predictions), dim=0)\n",
    "mse = loss_fn(y_pred_mean, y_test)\n",
    "print(f'Test MSE: {mse.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68a6035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuyin.wang\\AppData\\Local\\Temp\\ipykernel_16420\\2809603975.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  noise_std = torch.sqrt(torch.tensor(2.0 * lr * preconditioner))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 1280 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    119\u001b[39m     loss = F.mse_loss(outputs, batch_y)\n\u001b[32m    120\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Store weights after burn-in period with thinning\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch > burn_in \u001b[38;5;129;01mand\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuyin.wang\\.pyenv\\pyenv-win\\versions\\3.12.9\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mpSGLD.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Add Gaussian noise for Langevin dynamics\u001b[39;00m\n\u001b[32m     89\u001b[39m noise_std = torch.sqrt(torch.tensor(\u001b[32m2.0\u001b[39m * lr * preconditioner))\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m noise = torch.normal(mean=\u001b[32m0.0\u001b[39m, std=\u001b[43mnoise_std\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, size=p.data.size())\n\u001b[32m     91\u001b[39m noise = torch.from_numpy(noise).float()\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: a Tensor with 1280 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features and target\n",
    "scaler_x = StandardScaler().fit(X_train)\n",
    "scaler_y = StandardScaler().fit(y_train)\n",
    "X_train = scaler_x.transform(X_train)\n",
    "X_test = scaler_x.transform(X_test)\n",
    "y_train = scaler_y.transform(y_train).flatten()\n",
    "y_test = scaler_y.transform(y_test).flatten()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Bayesian Neural Network Architecture\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Custom pSGLD Optimizer (Preconditioned SGLD)\n",
    "class pSGLD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps, weight_decay=weight_decay)\n",
    "        super(pSGLD, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                \n",
    "                # Initialize state\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "                \n",
    "                state['step'] += 1\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "                lr = group['lr']\n",
    "                eps = group['eps']\n",
    "                weight_decay = group['weight_decay']\n",
    "                \n",
    "                # Add weight decay (Gaussian prior)\n",
    "                if weight_decay != 0:\n",
    "                    grad.add_(p.data, alpha=weight_decay)\n",
    "                \n",
    "                # Update squared gradient average\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                \n",
    "                # Compute preconditioner (RMS)\n",
    "                preconditioner = 1.0 / (torch.sqrt(square_avg) + eps)\n",
    "                \n",
    "                # Add Gaussian noise for Langevin dynamics\n",
    "                noise_std = torch.sqrt(torch.tensor(2.0 * lr * preconditioner))\n",
    "                noise = torch.normal(mean=0.0, std=noise_std.item(), size=p.data.size())\n",
    "                noise = torch.from_numpy(noise).float()\n",
    "                \n",
    "                # Update parameters\n",
    "                p.data.add_(-lr * 0.5 * preconditioner * grad + noise)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_epochs = 500\n",
    "burn_in = 100  # Discard first 100 samples\n",
    "num_samples = 50  # Posterior samples to keep\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer = pSGLD(model.parameters(), lr=1e-3, alpha=0.99, weight_decay=1e-4)\n",
    "\n",
    "# Training with MCMC sampling\n",
    "model.train()\n",
    "samples = []  # Store weight samples for Bayesian averaging\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x).flatten()\n",
    "        loss = F.mse_loss(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Store weights after burn-in period with thinning\n",
    "    if epoch > burn_in and epoch % 10 == 0:\n",
    "        samples.append({k: v.detach().clone().cpu() for k, v in model.state_dict().items()})\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "\n",
    "# Bayesian Model Averaging for Prediction\n",
    "def predict_bma(model, samples, dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_preds = []\n",
    "            \n",
    "            # Sample predictions\n",
    "            for sample in samples:\n",
    "                model.load_state_dict(sample)\n",
    "                outputs = model(batch_x).flatten().cpu().numpy()\n",
    "                batch_preds.append(outputs)\n",
    "            \n",
    "            # Calculate mean and std across samples\n",
    "            batch_preds = np.array(batch_preds)\n",
    "            pred_mean = np.mean(batch_preds, axis=0)\n",
    "            preds.append(pred_mean)\n",
    "    \n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# Generate predictions and evaluate\n",
    "y_pred = predict_bma(model, samples, test_loader)\n",
    "test_mse = np.mean((y_pred - y_test) ** 2)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# Uncertainty quantification example\n",
    "print(\"\\nUncertainty Quantification (First 5 test points):\")\n",
    "for i in range(5):\n",
    "    print(f\"True: {y_test[i]:.3f}, Pred: {y_pred[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968ae9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
