"""
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| Model                           | Pros                                                        | Cons                                                        |
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| LSTM (Long Short-Term Memory)   | - Handles long-range dependencies well.                     | - Computationally expensive for long sequences.             |
|                                 | - Mitigates vanishing gradient problem.                     | - Slower training due to more parameters.                   |
|                                 | - Suitable for complex sequences (e.g., time-series, text). |                                                             |
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| GRU (Gated Recurrent Unit)      | - Simpler architecture (fewer parameters).                  | - Struggles with very long sequences compared to LSTM.      |
|                                 | - Faster training than LSTM, often similar performance.     | - Fewer gates may limit flexibility for complex tasks.      |
|                                 | - Effective at learning temporal dependencies.              |                                                             |
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| RNN (Recurrent Neural Network)  | - Simple and easy to implement.                             | - Struggles with long-range dependencies(vanishing gradient)|
|                                 | - Good for short-term dependencies.                         | - Not suitable for complex tasks without enhancements.      |
|                                 | - Lower computational cost than LSTM/GRU.                   |                                                             |
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| BiLSTM (Bidirectional LSTM)     | - Captures both past and future context.                    | - More computationally expensive than unidirectional LSTM.  |
|                                 | - Suitable for tasks where future context is important.     | - Increases train time due to processing in both directions.|
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| 1D CNN (1D Convolutional        | - Efficient for local patterns in sequential data.          | - Limited at capturing long-term deps comparing to RNNs.    |
| Neural Networks)                |                                                             |                                                             |
|                                 | - Faster than RNNs for certain cases.                       | - Requires tuning of kernel size for optimal performance.   |
|                                 | - Can handle high-dimensional inputs.                       |                                                             |
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+
| Attention Mechanisms            | - Highly parallelizable, faster training.                   | - Requires large amounts of data for effective training.    |
| (e.g., Transformer)             |                                                             |                                                             |
|                                 | - Great at capturing long-range dependencies.               | - Computationally intensive with long sequences.            |
|                                 | - Scalable to large datasets and sequences.                 | - Complexity in hyperparameter tuning.                      |
+---------------------------------+-------------------------------------------------------------+-------------------------------------------------------------+

+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| Normalization Type     | Formula                                          | Best Used For                              | Characteristics                                  |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| Z-Score Normalization  | (x - Î¼) / Ïƒ                                      | - Normally distributed data                | - Mean = 0                                       |
| (StandardScaler)       | Transforms to mean 0, std dev 1                  | - Features on similar scales               | - Std Dev = 1                                    |
|                        |                                                  | - When outliers are not extreme            | - Sensitive to outliers                          |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| Min-Max Scaling        | (x - min(x)) / (max(x) - min(x))                 | - When you need bounded range              | - Preserves zero values                          |
| (MinMaxScaler)         | Scales to fixed range (default [0,1])            | - Neural network inputs                    | - Sensitive to outliers                          |
|                        |                                                  | - Image processing                         | - Does not handle new min/max in test data       |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| Robust Scaling         | (x - median) / IQR                               | - Data with many outliers                  | - Uses median instead of mean                    |
| (RobustScaler)         | Uses median and interquartile range              | - Skewed distributions                     | - Less affected by extreme values                |
|                        |                                                  | - Financial data with extreme values       | - Preserves shape of distribution                |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| MaxAbs Scaling         | x / max(abs(x))                                  | - Sparse data                              | - Preserves sign of original values              |
| (MaxAbsScaler)         | Scales by maximum absolute value                 | - When zero is meaningful                  | - Does not center data                           |
|                        |                                                  | - Machine learning with sparse features    | - Bounded between -1 and 1                       |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| Quantile Transformation| Transforms to uniform/normal distribution        | - Non-gaussian distributions               | - Makes data look like normal distribution       |
| (QuantileTransformer)  | Equalizes feature distributions                  | - When feature distributions differ        | - Can handle non-linear transformations          |
|                        |                                                  | - Machine learning with varied features    | - Destroys sparseness                            |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
| Power Transformation   | x^Î» or log(x)                                    | - Right-skewed data                        | - Stabilizes variance                            |
| (PowerTransformer)     | Stabilizes variance and makes data more Gaussian | - Financial ratios                         | - Handles positive values                        |
|                        |                                                  | - Economic indicators                      | - Different methods like Yeo-Johnson, Box-Cox    |
+------------------------+--------------------------------------------------+--------------------------------------------+--------------------------------------------------+
"""

"""
FEATURE PROPERTIES FOR ENSEMBLE VS NEURAL NETWORK METHODS
+-----------------------+--------------------------------+--------------------------------+
| Property              | Ensemble/Tree-Based Methods    | Neural Networks                |
+-----------------------+--------------------------------+--------------------------------+
| Value Range           | - Raw values acceptable        | - Scale to [-1, 1] or [0, 1]   |
|                       | - No scaling needed            | - Min-max or standard scaling  |
|                       | - Unbounded OK                 | - Remove values beyond Â±3Ïƒ     |
+-----------------------+--------------------------------+--------------------------------+
| Distribution          | - Any distribution OK          | - Normal/Uniform preferred     |
|                       | - Multimodal OK                | - Transform skewed features    |
|                       | - No transformation needed     | - Log/Box-Cox for heavy tails  |
+-----------------------+--------------------------------+--------------------------------+
| Mean                  | - Any value OK                 | - ~0 after standardization     |
|                       | - No centering needed          | - Center before training       |
+-----------------------+--------------------------------+--------------------------------+
| Variance              | - Non-zero required            | - ~1 after standardization     |
|                       | - No upper bound               | - Similar scale across features|
|                       | - High variance OK             | - Bounded variance preferred   |
+-----------------------+--------------------------------+--------------------------------+
| Skewness              | - Any value acceptable         | - Between -1 and 1             |
|                       | - No transformation needed     | - Transform if outside range   |
|                       | - Handles asymmetry well       | - Symmetric preferred          |
+-----------------------+--------------------------------+--------------------------------+
| Kurtosis              | - Any value acceptable         | - Close to 3 (normal dist)     |
|                       | - Handles heavy tails well     | - Transform heavy tails        |
|                       | - No transformation needed     | - Avoid extreme peaks          |
+-----------------------+--------------------------------+--------------------------------+
| Missing Values        | - Up to 25-30% acceptable      | - Less than 5% preferred       |
|                       | - Handle natively              | - Requires imputation          |
|                       | - Can use as split point       | - Use mean/median/mode         |
+-----------------------+--------------------------------+--------------------------------+
| Outliers              | - Robust to outliers           | - Remove or cap at Â±3Ïƒ         |
|                       | - Can keep as is               | - Use robust scaling           |
|                       | - May improve splits           | - Consider Winsorization       |
+-----------------------+--------------------------------+--------------------------------+
| Cardinality           | - High cardinality OK          | - Low cardinality preferred    |
| (Categorical)         | - Up to 50-100 levels OK       | - One-hot if < 15 levels       |
|                       | - Can handle rare cats         | - Embedding for high cardinal  |
+-----------------------+--------------------------------+--------------------------------+
| Interactions          | - Auto-detected                | - Need manual engineering      |
|                       | - No preprocessing needed      | - Cross-features helpful       |
|                       | - Handles non-linearity        | - Polynomial features help     |
+-----------------------+--------------------------------+--------------------------------+
| Feature Independence  | - Handles correlation well     | - Independent preferred        |
|                       | - No decorrelation needed      | - PCA/decorrelation helps      |
|                       | - Redundancy OK                | - Remove highly correlated     |
+-----------------------+--------------------------------+--------------------------------+
| Zero/Constant Values  | - Auto-filtered                | - Remove constant features     |
|                       | - Low variance OK              | - Remove near-zero variance    |
|                       | - Sparse data OK               | - Sparse inputs challenging    |
+-----------------------+--------------------------------+--------------------------------+
| Numeric Resolution    | - Any precision OK             | - 32-bit float typical         |
|                       | - Integer or float OK          | - Consistent precision needed  |
|                       | - Binary values OK             | - Normalize if large scale diff|
+-----------------------+--------------------------------+--------------------------------+
| Feature Scale         | - Any range acceptable         | - Similar scales crucial       |
| Relationships         | - No relative scaling needed   | - Normalize across features    |
|                       | - Preserves relationships      | - Balance feature importance   |
+-----------------------+--------------------------------+--------------------------------+
"""

"""
WEAKNESSES OF ENSEMBLE TREE-BASED METHODS
+---------------------------+------------------------------------------------+
| Category                  | Limitations & Issues                           |
+---------------------------+------------------------------------------------+
| Computational Resources   | - Large memory footprint for model storage     |
|                           | - Slow inference with deep trees               |
|                           | - Training time scales poorly (N*logN)         |
|                           | - High RAM usage during training               |
|                           | - Parallel training limited by CPU cores       |
+---------------------------+------------------------------------------------+
| Feature Space Handling    | - Inefficient with sparse high-dim features    |
|                           | - Cannot learn natural linear combinations     |
|                           | - Manual feature crossing often needed         |
|                           | - Many trees needed for simple linear patterns |
|                           | - Poor handling of cyclic/periodic features    |
+---------------------------+------------------------------------------------+
| Optimization & Training   | - Discrete optimization (no smooth gradients)  |
|                           | - No end-to-end training capability            |
|                           | - Hard to incorporate domain constraints       |
|                           | - Limited transfer learning ability            |
|                           | - Cannot freeze/fine-tune partial model        |
+---------------------------+------------------------------------------------+
| Generalization Issues     | - Cannot extrapolate beyond training range     |
|                           | - Poor performance in sparse regions           |
|                           | - Rectangular decision boundaries only         |
|                           | - Limited smooth boundary approximation        |
|                           | - Struggles with continuous symmetries         |
+---------------------------+------------------------------------------------+
| Statistical Limitations   | - No native probability calibration            |
|                           | - Uncertainty requires special methods         |
|                           | - Cannot incorporate Bayesian priors           |
|                           | - Biased importance for correlated features    |
|                           | - No natural confidence intervals              |
+---------------------------+------------------------------------------------+
| Performance Ceiling       | - Plateaus with increased data size            |
|                           | - Exponential data needs with dimensions       |
|                           | - Cannot learn certain function classes        |
|                           | - Limited representation power                 |
|                           | - Inefficient for some pattern types           |
+---------------------------+------------------------------------------------+
| Model Properties          | - Prone to overfitting with deep trees         |
|                           | - Hard to interpret individual predictions     |
|                           | - No natural online learning                   |
|                           | - Model size grows with complexity             |
|                           | - Limited model compression options            |
+---------------------------+------------------------------------------------+
| Deployment Challenges     | - Large deployment package size                |
|                           | - High latency for real-time inference         |
|                           | - Resource-intensive serving                   |
|                           | - Complex versioning with ensembles            |
|                           | - Difficult to deploy on edge devices          |
+---------------------------+------------------------------------------------+
"""

"""
https://github.com/UFund-Me/Qbot

ç»å…¸ç­–ç•¥
äº¤æ˜“å¯¹è±¡	é€‰è‚¡	æ‹©æ—¶	é£Žé™©æŽ§åˆ¶ (ç»„åˆã€ä»“ä½ç®¡ç†)
è‚¡ç¥¨/æœŸè´§/è™šæ‹Ÿè´§å¸	
å¸ƒæž—çº¿å‡å€¼å›žå½’ ('2022)
ç§»åŠ¨å‡çº¿+KDJ
ç®€å•ç§»åŠ¨å‡çº¿
æƒ…ç»ªæŒ‡æ ‡ARBR
é˜¿éš†æŒ‡æ ‡(è¶‹åŠ¿äº¤æ˜“)
LightGBM é¢„æµ‹
SVM é¢„æµ‹
LSTMæ—¶åºé¢„æµ‹
å¼ºåŒ–å­¦ä¹ é¢„æµ‹
Q-Leaningé¢„æµ‹
éšæœºæ£®æž—é¢„æµ‹
RSIèƒŒç¦»ç­–ç•¥
éº»é›€ä¼˜åŒ–ç®—æ³•SSA
éšæœºç›¸å¯¹å¼ºå¼±æŒ‡æ•° StochRSI
å°å¸‚å€¼ ('2021)
å¸‚åœºä½Žä¼°å€¼ç­–ç•¥
RSRSæ‹©æ—¶
é…å¯¹äº¤æ˜“
ä¼ ç»ŸæŒ‡æ ‡ï¼ˆå¯¹åº”ä¸‹æ–¹Qbotæ”¯æŒçš„æŒ‡æ ‡ è¿™é‡Œï¼‰
å¸ƒæž—çº¿å‡å€¼å›žå½’ ('2022)
ç§»åŠ¨å‡çº¿+KDJ
ç®€å•ç§»åŠ¨å‡çº¿
åŒå‡çº¿ç­–ç•¥ ('2022)
æƒ…ç»ªæŒ‡æ ‡ARBR
é˜¿éš†æŒ‡æ ‡(è¶‹åŠ¿äº¤æ˜“)
LightGBM é¢„æµ‹
SVM é¢„æµ‹
LSTMæ—¶åºé¢„æµ‹
å¼ºåŒ–å­¦ä¹ é¢„æµ‹
Q-Leaningé¢„æµ‹
éšæœºæ£®æž—é¢„æµ‹
RSIèƒŒç¦»ç­–ç•¥
éº»é›€ä¼˜åŒ–ç®—æ³•SSA
éšæœºç›¸å¯¹å¼ºå¼±æŒ‡æ•° StochRSI
å› å­ç»„åˆ
RSIå’ŒCCIç»„åˆ
MACDå’ŒADXæŒ‡æ ‡
MACDå’ŒKDJæŒ‡æ ‡
å¤šå› å­äº¤æ˜“
alphalenså¤šå› å­äº¤æ˜“
å¤šç­–ç•¥æ•´åˆ
ç»„åˆç­–ç•¥
æŒ‡æ•°å¢žå¼º ('2022)
ç»å…¸ç­–ç•¥
å¤šå› å­é€‰è‚¡ ('2023)
æŒ‡æ•°å¢žå¼º ('2022)
Alphaå¯¹å†² ('2022)
ç½‘æ ¼äº¤æ˜“
åŒå‡çº¿ç­–ç•¥ ('2022)
æ‹ç‚¹äº¤æ˜“ ('2022)
è¶‹åŠ¿äº¤æ˜“
æµ·é¾Ÿç­–ç•¥
åŠ¨æ€å¹³è¡¡ç­–ç•¥
Kurtosis Portfolioç»„åˆç­–ç•¥ ('2023)
æŒ‡æ•°å¢žå¼º ('2022)
Alphaå¯¹å†² ('2022)
åŠ¨æ€å¹³è¡¡ç­–ç•¥
å¤šå› å­è‡ªåŠ¨ç»„åˆç­–ç•¥
åŸºé‡‘	
4433æ³•åˆ™ ('2022)
å¯¹å†²ç­–ç•¥ï¼šæŒ‡æ•°åž‹+å€ºåˆ¸åž‹å¯¹å†²
ç»„åˆç­–ç•¥ï¼šå¤šå› å­ç»„åˆé…ç½®
ç»„åˆç­–ç•¥ï¼šæƒ èµ¢æ™ºèƒ½ç®—æ³•1
ç»„åˆç­–ç•¥ï¼šæ‹©æ—¶å¤šç­–ç•¥
ç»„åˆç­–ç•¥ï¼šæ™ºèµ¢å¤šå› å­1
åŒä¸Š
æ™ºèƒ½ç­–ç•¥
GBDT	RNN	Reinforcement Learning	ðŸ”¥ Transformer	ðŸ”¥ LLM
GBDT
XGBoost (KDD'2016)
LightGBM (NIPS'2017)
Catboost (NIPS'2018)
BOOST
DoubleEnsemble (ICDM'2020)
TabNet (ECCV'2022)
LR
Line Regression ('2020)
CNN
MLP (CVPRW'2020)
GRU (ICCVW'2021)
ImVoxelNet (WACV'2022)
TabNet (AAAI'2019)
RNN
LSTM (Neural Computation'2017)
ALSTM (IJCAI'2022)
ADARNN (KDD'2021)
ADD (CoRL'2020)
KRNN ()
Sandwich ()
TFT (IJoF'2019)
GATs (NIPS'2017)
SFM (KDD'2017)
Transformer (NeurIPS'2017)
TCTS (ICML'2021)
TRA (KDD'2021)
TCN (KDD'2018)
IGMTF (KDD'2021)
HIST (KDD'2018)
Localformer ('2021)
ChatGPT
FinGPT
Benchmark and Model zoo
Results and models are available in the model zoo. AI strategies is shown at here, local run python backend/pytrader/strategies/workflow_by_code.py, also provide Binder

ðŸ‘‰ ç‚¹å‡»å±•å¼€æŸ¥çœ‹å…·ä½“AIæ¨¡åž‹benchmarkç»“æžœ

äº¤æ˜“æŒ‡æ ‡/å› å­
åŒ…å«ä½†ä¸é™äºŽalpha-101ã€alpha-191ï¼Œä»¥åŠåŸºäºŽdeapå®žçŽ°çš„å› å­è‡ªåŠ¨ç”Ÿæˆç®—æ³•ã€‚

EMA(ç®€å•ç§»åŠ¨å‡çº¿)
MACD(æŒ‡æ•°å¹³æ»‘å¼‚åŒå¹³å‡çº¿)
KDJ(éšæœºæŒ‡æ ‡)
RSRS(é˜»åŠ›æ”¯æ’‘ç›¸å¯¹å¼ºåº¦)
RSI(ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡)
StochRSI(éšæœºç›¸å¯¹å¼ºå¼±æŒ‡æ•°)
BIAS(ä¹–ç¦»çŽ‡)
BOLL(å¸ƒæž—çº¿æŒ‡æ ‡)
OBV(èƒ½é‡æ½®)
SAR(æŠ›ç‰©è½¬å‘)
VOL(æˆäº¤é‡)
PSY(å¿ƒç†çº¿)
ARBR(äººæ°”å’Œæ„æ„¿æŒ‡æ ‡)
CR(å¸¦çŠ¶èƒ½åŠ›çº¿)
BBI(å¤šç©ºæŒ‡æ ‡)
EMV(ç®€æ˜“æ³¢åŠ¨æŒ‡æ ‡)
TRIX(ä¸‰é‡æŒ‡æ•°å¹³æ»‘ç§»åŠ¨å¹³å‡æŒ‡æ ‡)
DMA(å¹³å‡çº¿å·®)
DMI(è¶‹å‘æŒ‡æ ‡)
CCI(é¡ºåŠ¿æŒ‡æ ‡)
ROC(å˜åŠ¨é€ŸçŽ‡æŒ‡æ ‡, å¨å»‰æŒ‡æ ‡)
ENE(è½¨é“çº¿)  # è½¨é“çº¿ï¼ˆENEï¼‰ç”±ä¸Šè½¨çº¿(UPPER)å’Œä¸‹è½¨çº¿(LOWER)åŠä¸­è½¨çº¿(ENE)ç»„æˆï¼Œ
            # è½¨é“çº¿çš„ä¼˜åŠ¿åœ¨äºŽå…¶ä¸ä»…å…·æœ‰è¶‹åŠ¿è½¨é“çš„ç ”åˆ¤åˆ†æžä½œç”¨ï¼Œä¹Ÿå¯ä»¥æ•é”çš„è§‰å¯Ÿè‚¡ä»·è¿è¡Œè¿‡ç¨‹ä¸­æ–¹å‘çš„æ”¹å˜
SKDJ(æ…¢é€ŸéšæœºæŒ‡æ ‡)
LWR(æ…¢é€Ÿå¨å»‰æŒ‡æ ‡)  # è¶‹åŠ¿åˆ¤æ–­æŒ‡æ ‡
å¸‚ç›ˆçŽ‡
å¸‚å‡€çŽ‡
ä¸»åŠ›æ„æ„¿(æ”¶è´¹)
ä¹°å–å·®(æ”¶è´¹)
æ•£æˆ·çº¿(æ”¶è´¹)
åˆ†æ—¶åšå¼ˆ(æ”¶è´¹)
ä¹°å–åŠ›é“(æ”¶è´¹)
è¡Œæƒ…è¶‹åŠ¿(æ”¶è´¹)
MTM(åŠ¨é‡è½®åŠ¨æŒ‡æ ‡)(æ”¶è´¹)
MACDæ™ºèƒ½å‚æ•°(æ”¶è´¹)
KDJæ™ºèƒ½å‚æ•°(æ”¶è´¹)
RSIæ™ºèƒ½å‚æ•°(æ”¶è´¹)
WRæ™ºèƒ½å‚æ•°(æ”¶è´¹)
Qbotæ™ºèƒ½é¢„æµ‹(æ”¶è´¹)
Qbotä¹°å–å¼ºå¼±æŒ‡æ ‡(æ”¶è´¹)
"""